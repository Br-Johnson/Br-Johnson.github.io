{
  "hash": "6821f5c2b96962439ca99bafd9082092",
  "result": {
    "markdown": "---\ntitle: How Does the Global Biodiversity Information System Count Citations from the International Year of the Salmon?\nauthor: Brett Johnson\ndate: 2023-06-20\nimage: 'images/IYS_data_citations.png'\ncategories: [Biodiversity, GBIF, Datacite, International Year of the Salmon]\ndescription: 'How to use the DataCite REST API to understand how GBIF counts citations'\narchives:\n  - 2023/06\ntoc: false\nformat:\n  html:\n    code-fold: true\n    code-tools: true\nbibliography: references.bib\n---\n\n\n# Introduction\n\nIf you publish your data to the [Global Biodiversity Information Facility (GBIF)](https://www.gbif.org/), you may notice that your dataset is being cited by papers that seemingly have nothing to do with the context of your original project that produced the dataset. For example, check out the [citations of this zooplankton dataset](https://www.gbif.org/resource/search?contentType=literature&gbifDatasetKey=d80c46be-b600-44d5-9758-ef5159d40002).\n\nI had a colleague involved with the [International Year of the Salmon](https://yearofthesalmon.org) (IYS) ask why this zooplankton dataset from the IYS expeditions was referenced in [a paper that seemed to be about terrestrial species](https://doi.org/10.1016/j.biocon.2023.110118) [@garcía-roselló2023]. It's not immediately obvious how the IYS zooplankton dataset was used in this paper, but if you dig in a little deeper it becomes clear, and frankly amazing, how GBIF integrates data and counts citations.\n\n### How was the IYS Zoop Data Used?\n\nIf you look in the references section of the Garcia-Rosello et al. (2023) paper you will see two links to GBIF Occurrence Downloads. If you click on the DOI for the second GBIF Occurrence Download dataset [@gbif.orguser2022], you will reach a landing page for an Occurrence Download on GBIF from a specific query made by a user. You can even see the specific query filters that were used to generate the dataset.\n\n[![The query filters generated by a GBIF user to return an integrated dataset](images/Screenshot%202023-06-28%20at%208.00.15%20AM.png)](https://www.gbif.org/occurrence/download/0008235-220831081235567)\n\nA total of 18,426 datasets that meet the query filter. The filter included: has CC BY-NC 4.0 licence, Basis of record must be either Human observation, Observation or Preserved Specimen, Has coordinate is true, ... Scientific name includes Animalia). As it turns out, this is a very broad filter basically asking for all the animal occurrences on GBIF datasets that have the right license and metadata and the IYS dataset about zooplankton met all those criteria. If you wanted to see all the datasets included in the new dataset you can download a list of the involved datasets using the \"Download as TSV\" link from the [GBIF Occurrence Dataset landing page](https://doi.org/10.15468/dl.udrufp) for this big dataset and search for International Year of the Salmon to see which datasets are included.\\\n\\\nSo the dataset that resulted from this specific query includes data from 18,426 other GBIF datasets which meet the query filter parameters. This new dataset receives a Digital Object Identifier and each of those underlying datasets also has a digital object identifier. GBIF is able to count citations because each of those 18,426 DOIs is referenced in the new dataset's DOI metadata as a 'relatedIdentifier'. So, each time the overarching dataset is cited, the citation count for each of the 18,426 datasets increases by one as well. Pretty cool, huh!?\\\n\nI can be surprising how the International Year of the Salmon data are re-used way outside the context of their collection, which is a major advantage of using the GBIF and publishing data using international standards. This also demonstrates the power of standardized data: new datasets can be integrated, downloaded, and identified with a DOI on the fly!\n\nIf you're interested in finding all 18,427 Digital Object Identfiers and their titles or other metadata here's how you could do that using the [DataCite API](https://support.datacite.org/docs/api) and the [rdatacite R package](https://docs.ropensci.org/rdatacite/index.html) and searching for the DOI of the overarching dataset and extracting the `relatedIdentifiers` field.\n\n### International Year of the Salmon Datasets\n\nFor a more interesting example, let's look at how all the International Year of the Salmon datasets have been cited thus far. To do that we'll take a similar approach but instead of searching for a specific DOI we'll query for International Year of the Salmon in all the datacite DOIs, and extract each datasets `relatedIdentifiers`. We'll then search for those `relatedIdentifiers` to retrieve their titles. Finally, I'll join all that data together and present a network map of how each dataset connected by citations.\n\n\n::: {.cell fig.margin='true' hash='2023-06-20-How-does-gbif-count-citations_cache/html/unnamed-chunk-1_2a08871d1ae85f987250f819584cf7a6'}\n\n```{.r .cell-code}\nlibrary(rdatacite)\nlibrary(tidyverse)\nlibrary(networkD3)\n\n# download the DOI metadata for the new overarching dataset\ngbif_doi <- dc_dois(ids = \"10.15468/dl.cqpa99\")\n\ndatasets_used <- gbif_doi[[\"data\"]][[\"attributes\"]][[\"relatedIdentifiers\"]][[1]][\"relatedIdentifier\"]\n\n# Get the data from 'International Year of the Salmon' titles\niys_dois <- dc_dois(query = \"titles.title:International Year of the Salmon\", limit = 1000)\n\n# Create a tibble with the title, citation count, and DOI for each record, then filter by citation count greater than 0\niys_citations <- tibble(\n  title = lapply(iys_dois$data$attributes$titles, \"[[\", \"title\"),\n  citations = iys_dois[[\"data\"]][[\"attributes\"]][[\"citationCount\"]],\n  doi = iys_dois[[\"data\"]][[\"attributes\"]][[\"doi\"]]\n) |> filter(citations > 0)\n\n# Reduce the title to the substring from the 4th to the 80th character\niys_citations$title <- substr(iys_citations$title, 4, 80)\n\n# Initialize a list to store citation details of each DOI\ncites_iys_list <- list()\n\n# Fetch citation details of each DOI and store it in the list\nfor (i in iys_citations$doi) {\n  x <- dc_events(obj_id = paste0(\"https://doi.org/\", i))\n  cites_iys_list[[i]] <- x\n}\n\n# Initialize lists to store objId and subjId\nobj_ids <- list()\nsubj_ids <- list()\n\n# Loop over the list to retrieve objId and subjId\nfor(i in 1:length(cites_iys_list)) {\n  data <- cites_iys_list[[i]]$data$attributes\n  obj_ids[[i]] <- data$objId\n  subj_ids[[i]] <- data$subjId\n}\n\n# Flatten the lists and remove the prefix 'https://doi.org/'\nobj_ids <- substring(unlist(obj_ids), 17)\nsubj_ids <- substring(unlist(subj_ids), 17)\n\n# Get titles for objId and subjId\nobj_titles <- rdatacite::dc_dois(ids = obj_ids, limit = 1000)\nsubj_titles <- rdatacite::dc_dois(ids = subj_ids, limit = 1000)\n\n# Create a tibble of position, obj_doi, and its corresponding title\nobj_dois <- obj_titles[[\"data\"]][[\"attributes\"]][[\"doi\"]]\ntitle_list <- obj_titles[[\"data\"]][[\"attributes\"]][[\"titles\"]]\ntitle_vector <- unlist(map(title_list, function(x) x[['title']][1]))\nseq <- as.character(1:length(obj_dois))\nobjects <- tibble(position = seq, obj_dois, title_vector)\n\n# Create a tibble of position, subj_doi, and its corresponding title\nsubj_dois <- subj_titles[[\"data\"]][[\"attributes\"]][[\"doi\"]]\nsubjtitle_list <- subj_titles[[\"data\"]][[\"attributes\"]][[\"titles\"]]\nsubjtitle_vector <- unlist(map(subjtitle_list, function(x) x[['title']][1]))\nseq2 <- as.character(1:length(subj_dois))\nsubjects <- tibble(seq2, subj_dois, subjtitle_vector) |> \n  filter(subjtitle_vector != \"Zooplankton Bongo Net Data from the 2019 and 2020 Gulf of Alaska International Year of the Salmon Expeditions\")\n\n# Get related identifiers and filter by obj_dois, join with subjects, and filter by relationType\nsubj_related_ids <- bind_rows(subj_titles[[\"data\"]][[\"attributes\"]][[\"relatedIdentifiers\"]], .id = \"position\") |> \n  semi_join(objects, by = c('relatedIdentifier' = 'obj_dois')) |> \n  left_join(subjects, by = c('position' = 'seq2')) |> \n  filter(relationType != \"IsPreviousVersionOf\")\n\n# Join objects and subj_related_ids by 'obj_dois' = 'relatedIdentifier'\nrelationships <- full_join(objects, subj_related_ids, by = c('obj_dois' = 'relatedIdentifier'))\n\n# Prepare data for network plot\nlibrary(networkD3)\n\nobjects$type.label <- \"IYS Dataset\"\nsubjects$type.label <- \"Referencing Dataset\"\nids <- c(objects$obj_dois,subjects$subj_dois)\nnames <- c(objects$title_vector, subjects$subjtitle_vector)\ntype.label <- c(objects$type.label, subjects$type.label)\n\n# Create edges for network plot\nedges <-tibble(from = relationships$obj_dois, to = relationships$subj_dois)\nlinks.d3 <- data.frame(from=as.numeric(factor(edges$from))-1, \n                       to=as.numeric(factor(edges$to))-1 ) \nsize <- links.d3 |> \n   group_by(from) |> \n  summarize(weight = n())\n\nnodes <- tibble(ids, names, type.label) |> \n  mutate(names = case_when(\n    names == \"Occurrence Download\" ~ paste0(names, \" \", ids),\n    TRUE ~ names\n    ),\n  )\n\nlength <- nrow(nodes)\n\nmissing_length <- as.integer(length) - nrow(size)\nmissing_size <- rep.int(0, missing_length)\nsize <- c(size$weight, missing_size)\n\nnodes$size <- size\n\nnodes.d3 <- cbind(idn=factor(nodes$names, levels=nodes$names), nodes) \n\n# Create and render the network plot\nlibrary(networkD3)\nplot <- forceNetwork(Links = links.d3, Nodes = nodes.d3, Source=\"from\", Target=\"to\",\n               NodeID = \"idn\", Group = \"type.label\",linkWidth = 1,\n               linkColour = \"#afafaf\", fontSize=12, zoom=T, legend=T, \n               Nodesize=\"size\", opacity = 0.8, charge=-300,\n               width = 600, height = 400)\n\nplot\n```\n\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-9e5ae7de6ffc939c247a\" style=\"width:100%;height:433px;\" class=\"forceNetwork html-widget \"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-9e5ae7de6ffc939c247a\">{\"x\":{\"links\":{\"source\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,7,7,7,7,7,7,7,7,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5],\"target\":[17,18,14,5,27,4,10,2,1,3,12,15,26,20,8,9,24,19,6,7,16,22,0,23,13,25,17,14,5,27,3,15,20,21,24,17,18,14,5,27,3,15,26,20,21,11,24,6,16,0,17,5,27,3,15,20,17,14,5,27,4,10,2,1,3,12,15,26,20,8,24,16,17,18,14,5,27,3,15,26,20,21,24,6,7,16,22,0,17,14,5,27,4,10,2,1,3,12,15,26,20,8,24,19,16,17,5,27,3,15,26,20,21,24,17,5,27,3,15,20,21,17,14,5,27,3,15,20,21,24],\"colour\":[\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\",\"#afafaf\"]},\"nodes\":{\"name\":[\"Zooplankton Bongo Net Data from the 2019 and 2020 Gulf of Alaska International Year of the Salmon Expeditions\",\"Trawl Data from the R/V TINRO during the 2022 International Year of the Salmon Pan-Pacific Winter High Seas Expedition\",\"Trawl Catch and Species Abundance data from the 2020 Gulf of Alaska International Year of the Salmon Expedition\",\"Bongo Zooplankton Data from the R/V TINRO, NOAA Bell M. Shimada and F/V Northwest Explorer during the 2022 International Year of the Salmon Pan-Pacific Winter High Seas Expedition\",\"Juday Net Zooplankton Data from the 2019 Gulf of Alaska International Year of the Salmon Expedition\",\"Trawl Catch and Species Abundance from the 2019 Gulf of Alaska International Year of the Salmon Expedition\",\"Juday Net Zooplankton Data from the 2020 Gulf of Alaska International Year of the Salmon Expedition\",\"Trawl Data from the CCGS Sir John Franklin during the 2022 International Year of the Salmon Pan-Pacific Winter High Seas Expedition\",\"Trawl Data from the FV Northwest Explorer during the 2022 International Year of the Salmon Pan-Pacific Winter High Seas Expedition\",\"Trawl Data from the NOAA Bell M. Shimada during the 2022 International Year of the Salmon Pan-Pacific Winter High Seas Expedition\",\"Occurrence Download 10.15468/dl.gx742r\",\"Occurrence Download 10.15468/dl.hz87ut\",\"Occurrence Download 10.15468/dl.cqpa99\",\"Occurrence Download 10.15468/dl.6vdkbn\",\"Occurrence Download 10.15468/dl.zycegr\",\"Occurrence Download 10.15468/dl.5bs8n8\",\"Occurrence Download 10.15468/dl.862h55\",\"Occurrence Download 10.15468/dl.3yf6de\",\"Occurrence Download 10.15468/dl.23aja3\",\"Occurrence Download 10.15468/dl.459uag\",\"Occurrence Download 10.15468/dl.ajb76z\",\"Occurrence Download 10.15468/dl.f8s2hg\",\"Occurrence Download 10.15468/dl.vaz2q7\",\"Occurrence Download 10.15468/dl.pk3trq\",\"Occurrence Download 10.15468/dl.7gzvbw\",\"Occurrence Download 10.15468/dl.rm8rn8\",\"Occurrence Download 10.15468/dl.a5mbxn\",\"Occurrence Download 10.15468/dl.7m3x6e\",\"Occurrence Download 10.15468/dl.ucfhd2\",\"Occurrence Download 10.15468/dl.mpsknf\",\"Occurrence Download 10.15468/dl.6vtqy8\",\"Occurrence Download 10.15468/dl.72zk7d\",\"Occurrence Download 10.15468/dl.fdqx94\",\"Occurrence Download 10.15468/dl.t4gc8j\",\"More than half of data deficient species predicted to be threatened by extinction\",\"Occurrence Download 10.15468/dl.tsup7a\",\"Occurrence Download 10.15468/dl.c3mkt8\",\"Occurrence Download 10.15468/dl.v4qqas\"],\"group\":[\"IYS Dataset\",\"IYS Dataset\",\"IYS Dataset\",\"IYS Dataset\",\"IYS Dataset\",\"IYS Dataset\",\"IYS Dataset\",\"IYS Dataset\",\"IYS Dataset\",\"IYS Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\",\"Referencing Dataset\"],\"nodesize\":[15,26,16,17,9,9,7,9,6,16,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]},\"options\":{\"NodeID\":\"idn\",\"Group\":\"type.label\",\"colourScale\":\"d3.scaleOrdinal(d3.schemeCategory20);\",\"fontSize\":12,\"fontFamily\":\"serif\",\"clickTextSize\":30,\"linkDistance\":50,\"linkWidth\":\"1\",\"charge\":-300,\"opacity\":0.8,\"zoom\":true,\"legend\":true,\"arrows\":false,\"nodesize\":true,\"radiusCalculation\":\" Math.sqrt(d.nodesize)+6\",\"bounded\":false,\"opacityNoHover\":0,\"clickAction\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nIn summary, the DataCite REST API offers a lot of great details about datasets published with a DOI. Using this service you can understand how your data are being used in a programmatic way that would be easy to create a dashboard with.\n\nOne limitation to Datacite's REST API, however, is that it only indexes DOIs minted by Datacite and not by other services such as CrossRef which mints DOIs mainly for journal articles.\n\nThankfully, DataCite also offers a [GraphQL API](https://support.datacite.org/docs/datacite-graphql-api-guide) which indexes not only DataCite and Crossref, but also [ORCID](https://orcid.org/), and [ROR](https://ror.org/)! So, stay tuned for a future blog post demonstrating the use of this amazing service.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/htmlwidgets-1.6.2/htmlwidgets.js\"></script>\n<script src=\"../../site_libs/d3-4.5.0/d3.min.js\"></script>\n<script src=\"../../site_libs/forceNetwork-binding-0.4/forceNetwork.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}