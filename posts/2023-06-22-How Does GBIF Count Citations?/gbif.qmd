---
title: How Does GBIF Count Citations?
author: Brett Johnson
date: 2023-06-22
categories: [How-to,R,Rmarkdown]
description: 'description'
archives:
  - 2023/06
toc: false

format:
  html:
    code-fold: true
    code-tools: true
---

# Introduction

If you publish your data to the Global Biodiversity Information Facility (GBIF), you may notice that your dataset is being cited by papers that seemingly have nothing to do with the context of your original project that produced the dataset.

I had a colleague involved with the [International Year of the Salmon](https://yearofthesalmon.org) (IYS) ask why IYS datasets such as one [dataset about zooplankton](https://www.gbif.org/dataset/d80c46be-b600-44d5-9758-ef5159d40002) was referenced in [a paper that seemed to be about terrestrial species](https://doi.org/10.1016/j.biocon.2023.110118) [@garcía-roselló2023].

If you look in the references of the Garcia-Rosello et al 2023 paper you will see two links to GBIF Occurrence Downloads. If you click on the DOI for GBIF.org, 2022b (<https://doi.org/10.15468/dl.cqpa99>), you will reach a landing page for an Occurrence Download from GBIF from a specific query. You can see the specific query filters that were used to generate the dataset.

A total of 18,426 datasets that meet the query filter. The filter included: has CC BY 4.0 licence, Basis of record must be either Human observation, Observation or Preserved Specimen, Has coordinate is true, ... Scientific name includes Animalia). As it turns out, this is a very broad filter basically asking for all the animal occurrences on GBIF datasets that have the right license and metadata and the dataset about zooplankton met all those criteria. If you wanted to see all the datasets included in the new dataset you can download a list of the involved datasets using the "Download as TSV" link from the [GBIF Occurrence Dataset landing page](https://doi.org/10.15468/dl.udrufp) for this big dataset and search for International Year of the Salmon to see which datasets are included.\
\
So the dataset that resulted from this specific query includes occurrences meeting the query filter parameters from 18,426 other GBIF datasets receives a Digital Object Identifier. Each of those underlying datasets also has a digital object identifier and each of those DOIs is referenced in the new dataset's DOI metadata as a 'relatedIdentifier'.\
\
So, each time the overarching dataset is cited, the citation count for each of the 18,426 datasets increases by one as well. Pretty cool, huh!?\

I can be surprising how the International Year of the Salmon data are re-used way outside the context of their collection, which is a major advantage of using the GBIF and publishing data using international standards.

Here's how you can access the metadata about the overarching DOI as well to see what kind of datasets it included.

```{r, warning = FALSE, message=FALSE}
library(rdatacite)
library(tidyverse)

# download the DOI metadata for the new overarching dataset
gbif_doi <- dc_dois(ids = "10.15468/dl.cqpa99")

datasets_used <- gbif_doi[["data"]][["attributes"]][["relatedIdentifiers"]][[1]]["relatedIdentifier"]

```

Using this list of DOIs that the overarching dataset uses, we could retrieve all the names of those datasets, but let's do something a bit more interesting.

Let's look at the International Year of the Salmon datasets and how they've been utilized by GBIF.

```{r, warning = FALSE, message=FALSE, cache=TRUE}

iys_dois <- dc_dois(query = "titles.title:International Year of the Salmon", limit = 1000)

iys_citations <- tibble(title = lapply(iys_dois$data$attributes$titles, "[[", "title"), citations = iys_dois[["data"]][["attributes"]][["citationCount"]], doi = iys_dois[["data"]][["attributes"]][["doi"]]) |> 
  filter(citations > 0)

iys_citations$title <- substr(iys_citations$title, 4, 80)

cites_iys_list <- list()

for (i in iys_citations$doi) {
  
  x <- dc_events(obj_id = paste0("https://doi.org/", i))
  cites_iys_list[[i]] <- x
}

# Initialize lists to store objId and subjId
obj_ids <- list()
subj_ids <- list()

# Loop over the list to retrieve objId and subjId
for(i in 1:length(cites_iys_list)) {
  data <- cites_iys_list[[i]]$data$attributes
  obj_ids[[i]] <- data$objId
  subj_ids[[i]] <- data$subjId
}

# Flatten the lists
obj_ids <- unlist(obj_ids)
obj_ids <- substring(obj_ids, 17)
subj_ids <- unlist(subj_ids)
subj_ids <- substring(subj_ids, 17)

# Get titles for objId and subjId
obj_titles <- rdatacite::dc_dois(ids = obj_ids, limit = 1000)
subj_titles <- rdatacite::dc_dois(ids = subj_ids, limit = 1000)

obj_dois <- obj_titles[["data"]][["attributes"]][["doi"]]
title_list <- obj_titles[["data"]][["attributes"]][["titles"]]
title_vector <- unlist(map(title_list, function(x) x[['title']][1]))
seq <- as.character(1:length(obj_dois))
objects <- tibble(position = seq, obj_dois, title_vector)

subj_dois <- subj_titles[["data"]][["attributes"]][["doi"]]
subjtitle_list <- subj_titles[["data"]][["attributes"]][["titles"]]
subjtitle_vector <- unlist(map(subjtitle_list, function(x) x[['title']][1]))
seq2 <- as.character(1:length(subj_dois))
subjects <- tibble(seq2, subj_dois, subjtitle_vector) |> 
  filter(subjtitle_vector != "Zooplankton Bongo Net Data from the 2019 and 2020 Gulf of Alaska International Year of the Salmon Expeditions")

subj_related_ids <- bind_rows(subj_titles[["data"]][["attributes"]][["relatedIdentifiers"]], .id = "position") |> 
  semi_join(objects, by = c('relatedIdentifier' = 'obj_dois')) |> 
  left_join(subjects, by = c('position' = 'seq2')) |> 
  filter(relationType != "IsPreviousVersionOf")
  


relationships <- full_join(objects, subj_related_ids, by = c('obj_dois' = 'relatedIdentifier'))


library(networkD3)

objects$type.label <- "IYS Dataset"
subjects$type.label <- "Referencing Dataset"
ids <- c(objects$obj_dois,subjects$subj_dois)
names <- c(objects$title_vector, subjects$subjtitle_vector)
type.label <- c(objects$type.label, subjects$type.label)

edges <-tibble(from = relationships$obj_dois, to = relationships$subj_dois)
links.d3 <- data.frame(from=as.numeric(factor(edges$from))-1, 
                       to=as.numeric(factor(edges$to))-1 ) 
size <- links.d3 |> 
   group_by(from) |> 
  summarize(weight = n())

nodes <- tibble(ids, names, type.label) |> 
  mutate(names = case_when(
    names == "Occurrence Download" ~ paste0(names, " ", ids),
    TRUE ~ names
    ),
  )

length <- nrow(nodes)

missing_length <- as.integer(length) - nrow(size)
missing_size <- rep.int(0, missing_length)
size <- c(size$weight, missing_size)

nodes$size <- size

nodes.d3 <- cbind(idn=factor(nodes$names, levels=nodes$names), nodes) 
```

```{r, warning = FALSE, message=FALSE}
library(networkD3)
plot <- forceNetwork(Links = links.d3, Nodes = nodes.d3, Source="from", Target="to",
               NodeID = "idn", Group = "type.label",linkWidth = 1,
               linkColour = "#afafaf", fontSize=12, zoom=T, legend=T, 
               Nodesize="size", opacity = 0.8, charge=-300,
               width = 600, height = 400)

plot
```
