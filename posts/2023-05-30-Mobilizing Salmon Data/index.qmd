---
title: How do we mobilize salmon data?
author: Brett Johnnson
date: 2023-05-30
categories: [Salmon, ecology]
description: 'Communicate, assemble and coordinate extant data systems'
image: ''
archives:
  - 2023/05
toc: false

format:
  html:
    code-fold: show
    code-tools: true
bibliography: references.bib
---

# Introduction

There exists a mountain of data about salmon, their environment, and abundance yet we struggle to understand why or predict when certain populations of salmon decline. Part of the challenge is that salmon have a complicated life history where they are exposed to multiple environments throughout their lives. Some parts of their lives are easier to observe than others and some environments are more complex than others. The challenge is to figure out what observations have the most predictive power in each life phase, and then put all those data together to glean a complete and predictive history of the conditions encountered by specific salmon stocks.

However, because salmon cross arbitrary municipal, provincial, and national borders we lack a coordinated approach to aggregating data. Data often are collected using bespoke standards, stored locally, and not shared. Data management is not a trivial task. Therefore, any coordinated approach needs to be lightweight and flexible such that the barrier to adherence is surmountable by individual biologists and fall in line with the basic tenets of FAIR data.

The truth is we don't need to develop any new technologies to mobilize salmon data, we need only implement the current technologies that exist, extend them where appropriate, integrate them where necessary, make them easy to use, communicate their existence, and make it clear when they must be used.

## Practical Steps to Mobilize Salmon Data

1.  **Publish data**: In the pursuit of transparency, academic journals should mandate data publication and citation for manuscripts. Similarly, institutions can require the delivery of published data.

2.  **Create a Data Management Plan (DMP)**: A robust DMP is crucial for the effective management of data. Fisheries and Oceans Canada already prescribes this requirement ([**https://www.dfo-mpo.gc.ca/about-notre-sujet/publications/science/datapolicy-politiquedonnees/index-eng.html#6-5**](https://www.dfo-mpo.gc.ca/about-notre-sujet/publications/science/datapolicy-politiquedonnees/index-eng.html#6-5)). The Government of Canada's Tri-agency Research Data Management Policy and the Canadian Association of Research Libraries Tri-agency supported [DMP Assistant tool](https://assistant.portagenetwork.ca/about_us) can aid in the creation of DMPs.

3.  **Publish metadata records**: Metadata records can be published on platforms like Open Canada Data Portal, the Canadian Integrated Ocean Observing System, the Global Biodiversity Information Facility and generalist repositories such as Dryad, Figshare and Zenodo. So long as the metadata catalogue mints Digital Object Identifiers and uses modern technologies that allow metadata to be indexed and harvested, such as [Science on Schema.org](https://doi.org/10.5281/zenodo.7872383), then bespoke collections can be harvested from a wide array of sources and integrated into a bespoke collection.

4.  **Publish Protocols**: The publication of protocols ensures uniformity in data collection and analysis. Several platforms exist to publish protocols for which a Digital Object Identifier can be used: [Nature Portfolio's Protocol Exchange](https://protocolexchange.researchsquare.com/), [protocol.io](https://www.protocols.io/plans/academia), or simply using GitHub to host versions of protocols (DOIs can be assigned via the [Zenodo Integration](https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content))

5.  **Use Persistent Identifiers**: Persistent Identifiers (PIDs) like Digital Object Identifiers (DOIs) for datasets, [Open Research and Contributor ID (ORCIDs)](https://orcid.org/) for individuals, and [Research Organization Registry RORs](https://ror.org/) for organizations can ensure precise, persistent and identification. Scientific Organizations can join the [DataCite Canada Consortium](https://www.crkn-rcdr.ca/en/datacite-canada-consortium) if they wish to mint their own DOIs or rely on free services such as [zenodo](https://zenodo.org/). Moreover, leveraging the nascent [PID graph](https://doi.org/10.1016/j.patter.2020.100180) to create custom applications or data visualizations that map bespoke research networks to gain an enhanced understanding of the research landscape.

6.  **Adopt domain-specific data and metadata standards**: Implementing domain-specific data and metadata standards, such as Darwin Core for biological data and Climate Forecast Conventions for oceanographic data, ensures uniformity and compatibility across data sets. For primary biodiversity data, the Canadian Journal of Aquatic Fisheries Sciences *already* strongly advocates for all species distribution records to be deposited in publicly accessible databases like the Global Biodiversity Information Facility (GBIF) nodes ([**www.gbif.org**](http://www.gbif.org/)), BioFresh ([**www.freshwaterbiodiversity.eu**](http://www.freshwaterbiodiversity.eu/)) for freshwater data, and the Ocean Biogeographic Information System (OBIS, [**http://www.iobis.org/**](http://www.iobis.org/)) for marine biodiversity data.

7.  **Promote and incentivize data publishing**: Encourage the scientific community to publish their data by offering incentives, rewards, and institutional recognition.

8.  **Promote and Support Reproducible Workflows**: Implement practices that ensure reproducibility of scientific workflows. This can be achieved by versioning data, code, and computational environments, and using workflow management systems that ensure the same computational procedure will yield the same result over time. This fosters trust, enables validation, and promotes the reuse of scientific workflows. Researchers should provide clear and comprehensive documentation of their workflows, including code, data, and tools used, to support reproducibility by others in the scientific community. Institutions should provide training on these emergent skills and best-practices.

9.  **Reuse data**: Promote the reuse of data, leveraging the vast amount of existing information to streamline new research and reduce duplication of efforts.

10. **Cite reused data**: Any reused data should be properly cited using its digital object identifier (DOI). This practice will ensure due acknowledgment to the original data creators and promote transparency ethical data usage, and credit for the data provider. Progressive academic journals are already taking [this approach](https://www.nature.com/articles/sdata2018259).

The process of aggregating and understanding salmon data has historically been fraught with difficulties due to the complicated life history of the species and the geographical and political boundaries that often hamper a coordinated approach. The need of the hour is a simplified, flexible, and coordinated strategy that is easy to implement and adheres to the FAIR principles. The effective mobilization of salmon data does not hinge on the creation of new technologies. Rather, the emphasis should be on the active communuication and implementation of existing technologies, ensuring they are easy to use and their use is mandated. The ten strategic points delineated here present a holistic roadmap towards this objective. With concerted efforts and strategic implementation of these proposed strategies, a comprehensive understanding of salmon population dynamics can become achievable.
