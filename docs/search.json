[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A blog about data, science, and technology."
  },
  {
    "objectID": "posts/2023-06-20-GBIF/2023-06-20-How-does-gbif-count-citations.html",
    "href": "posts/2023-06-20-GBIF/2023-06-20-How-does-gbif-count-citations.html",
    "title": "How Does the Global Biodiversity Information System Count Citations from the International Year of the Salmon?",
    "section": "",
    "text": "Introduction\nIf you publish your data to the Global Biodiversity Information Facility (GBIF), you may notice that your dataset is being cited by papers that seemingly have nothing to do with the context of your original project that produced the dataset. For example, check out the citations of this zooplankton dataset.\nI had a colleague involved with the International Year of the Salmon (IYS) ask why this zooplankton dataset from the IYS expeditions was referenced in a paper that seemed to be about terrestrial species (garcía-roselló2023?). It’s not immediately obvious how the IYS zooplankton dataset was used in this paper, but if you dig in a little deeper it becomes clear, and frankly amazing, how GBIF integrates data and counts citations.\n\nHow was the IYS Zoop Data Used?\nIf you look in the references section of the Garcia-Rosello et al. (2023) paper you will see two links to GBIF Occurrence Downloads. If you click on the DOI for the second GBIF Occurrence Download dataset (GBIF.Org User 2022), you will reach a landing page for an Occurrence Download on GBIF from a specific query made by a user. You can even see the specific query filters that were used to generate the dataset.\n\n\n\nThe query filters generated by a GBIF user to return an integrated dataset\n\n\nA total of 18,426 datasets that meet the query filter. The filter included: has CC BY-NC 4.0 licence, Basis of record must be either Human observation, Observation or Preserved Specimen, Has coordinate is true, … Scientific name includes Animalia). As it turns out, this is a very broad filter basically asking for all the animal occurrences on GBIF datasets that have the right license and metadata and the IYS dataset about zooplankton met all those criteria. If you wanted to see all the datasets included in the new dataset you can download a list of the involved datasets using the “Download as TSV” link from the GBIF Occurrence Dataset landing page for this big dataset and search for International Year of the Salmon to see which datasets are included.\n\nSo the dataset that resulted from this specific query includes data from 18,426 other GBIF datasets which meet the query filter parameters. This new dataset receives a Digital Object Identifier and each of those underlying datasets also has a digital object identifier. GBIF is able to count citations because each of those 18,426 DOIs is referenced in the new dataset’s DOI metadata as a ‘relatedIdentifier’. So, each time the overarching dataset is cited, the citation count for each of the 18,426 datasets increases by one as well. Pretty cool, huh!?\n\nI can be surprising how the International Year of the Salmon data are re-used way outside the context of their collection, which is a major advantage of using the GBIF and publishing data using international standards. This also demonstrates the power of standardized data: new datasets can be integrated, downloaded, and identified with a DOI on the fly!\nIf you’re interested in finding all 18,427 Digital Object Identfiers and their titles or other metadata here’s how you could do that using the DataCite API and the rdatacite R package and searching for the DOI of the overarching dataset and extracting the relatedIdentifiers field.\n\n\nInternational Year of the Salmon Datasets\nFor a more interesting example, let’s look at how all the International Year of the Salmon datasets have been cited thus far. To do that we’ll take a similar approach but instead of searching for a specific DOI we’ll query for International Year of the Salmon in all the datacite DOIs, and extract each datasets relatedIdentifiers. We’ll then search for those relatedIdentifiers to retrieve their titles. Finally, I’ll join all that data together and present a network map of how each dataset connected by citations.\n\n\nCode\nlibrary(rdatacite)\nlibrary(tidyverse)\nlibrary(networkD3)\n\n# download the DOI metadata for the new overarching dataset\ngbif_doi &lt;- dc_dois(ids = \"10.15468/dl.cqpa99\")\n\ndatasets_used &lt;- gbif_doi[[\"data\"]][[\"attributes\"]][[\"relatedIdentifiers\"]][[1]][\"relatedIdentifier\"]\n\n# Get the data from 'International Year of the Salmon' titles\niys_dois &lt;- dc_dois(query = \"titles.title:International Year of the Salmon\", limit = 1000)\n\n# Create a tibble with the title, citation count, and DOI for each record, then filter by citation count greater than 0\niys_citations &lt;- tibble(\n  title = lapply(iys_dois$data$attributes$titles, \"[[\", \"title\"),\n  citations = iys_dois[[\"data\"]][[\"attributes\"]][[\"citationCount\"]],\n  doi = iys_dois[[\"data\"]][[\"attributes\"]][[\"doi\"]]\n) |&gt; filter(citations &gt; 0)\n\n# Reduce the title to the substring from the 4th to the 80th character\niys_citations$title &lt;- substr(iys_citations$title, 4, 80)\n\n# Initialize a list to store citation details of each DOI\ncites_iys_list &lt;- list()\n\n# Fetch citation details of each DOI and store it in the list\nfor (i in iys_citations$doi) {\n  x &lt;- dc_events(obj_id = paste0(\"https://doi.org/\", i))\n  cites_iys_list[[i]] &lt;- x\n}\n\n# Initialize lists to store objId and subjId\nobj_ids &lt;- list()\nsubj_ids &lt;- list()\n\n# Loop over the list to retrieve objId and subjId\nfor(i in 1:length(cites_iys_list)) {\n  data &lt;- cites_iys_list[[i]]$data$attributes\n  obj_ids[[i]] &lt;- data$objId\n  subj_ids[[i]] &lt;- data$subjId\n}\n\n# Flatten the lists and remove the prefix 'https://doi.org/'\nobj_ids &lt;- substring(unlist(obj_ids), 17)\nsubj_ids &lt;- substring(unlist(subj_ids), 17)\n\n# Get titles for objId and subjId\nobj_titles &lt;- rdatacite::dc_dois(ids = obj_ids, limit = 1000)\nsubj_titles &lt;- rdatacite::dc_dois(ids = subj_ids, limit = 1000)\n\n# Create a tibble of position, obj_doi, and its corresponding title\nobj_dois &lt;- obj_titles[[\"data\"]][[\"attributes\"]][[\"doi\"]]\ntitle_list &lt;- obj_titles[[\"data\"]][[\"attributes\"]][[\"titles\"]]\ntitle_vector &lt;- unlist(map(title_list, function(x) x[['title']][1]))\nseq &lt;- as.character(1:length(obj_dois))\nobjects &lt;- tibble(position = seq, obj_dois, title_vector)\n\n# Create a tibble of position, subj_doi, and its corresponding title\nsubj_dois &lt;- subj_titles[[\"data\"]][[\"attributes\"]][[\"doi\"]]\nsubjtitle_list &lt;- subj_titles[[\"data\"]][[\"attributes\"]][[\"titles\"]]\nsubjtitle_vector &lt;- unlist(map(subjtitle_list, function(x) x[['title']][1]))\nseq2 &lt;- as.character(1:length(subj_dois))\nsubjects &lt;- tibble(seq2, subj_dois, subjtitle_vector) |&gt; \n  filter(subjtitle_vector != \"Zooplankton Bongo Net Data from the 2019 and 2020 Gulf of Alaska International Year of the Salmon Expeditions\")\n\n# Get related identifiers and filter by obj_dois, join with subjects, and filter by relationType\nsubj_related_ids &lt;- bind_rows(subj_titles[[\"data\"]][[\"attributes\"]][[\"relatedIdentifiers\"]], .id = \"position\") |&gt; \n  semi_join(objects, by = c('relatedIdentifier' = 'obj_dois')) |&gt; \n  left_join(subjects, by = c('position' = 'seq2')) |&gt; \n  filter(relationType != \"IsPreviousVersionOf\")\n\n# Join objects and subj_related_ids by 'obj_dois' = 'relatedIdentifier'\nrelationships &lt;- full_join(objects, subj_related_ids, by = c('obj_dois' = 'relatedIdentifier'))\n\n# Prepare data for network plot\nlibrary(networkD3)\n\nobjects$type.label &lt;- \"IYS Dataset\"\nsubjects$type.label &lt;- \"Referencing Dataset\"\nids &lt;- c(objects$obj_dois,subjects$subj_dois)\nnames &lt;- c(objects$title_vector, subjects$subjtitle_vector)\ntype.label &lt;- c(objects$type.label, subjects$type.label)\n\n# Create edges for network plot\nedges &lt;-tibble(from = relationships$obj_dois, to = relationships$subj_dois)\nlinks.d3 &lt;- data.frame(from=as.numeric(factor(edges$from))-1, \n                       to=as.numeric(factor(edges$to))-1 ) \nsize &lt;- links.d3 |&gt; \n   group_by(from) |&gt; \n  summarize(weight = n())\n\nnodes &lt;- tibble(ids, names, type.label) |&gt; \n  mutate(names = case_when(\n    names == \"Occurrence Download\" ~ paste0(names, \" \", ids),\n    TRUE ~ names\n    ),\n  )\n\nlength &lt;- nrow(nodes)\n\nmissing_length &lt;- as.integer(length) - nrow(size)\nmissing_size &lt;- rep.int(0, missing_length)\nsize &lt;- c(size$weight, missing_size)\n\nnodes$size &lt;- size\n\nnodes.d3 &lt;- cbind(idn=factor(nodes$names, levels=nodes$names), nodes) \n\n# Create and render the network plot\nlibrary(networkD3)\nplot &lt;- forceNetwork(Links = links.d3, Nodes = nodes.d3, Source=\"from\", Target=\"to\",\n               NodeID = \"idn\", Group = \"type.label\",linkWidth = 1,\n               linkColour = \"#afafaf\", fontSize=12, zoom=T, legend=T, \n               Nodesize=\"size\", opacity = 0.8, charge=-300,\n               width = 600, height = 400)\n\nplot\n\n\n\n\n\n\nIn summary, the DataCite REST API offers a lot of great details about datasets published with a DOI. Using this service you can understand how your data are being used in a programmatic way that would be easy to create a dashboard with.\nOne limitation to Datacite’s REST API, however, is that it only indexes DOIs minted by Datacite and not by other services such as CrossRef which mints DOIs mainly for journal articles.\nThankfully, DataCite also offers a GraphQL API which indexes not only DataCite and Crossref, but also ORCID, and ROR! So, stay tuned for a future blog post demonstrating the use of this amazing service.\n\n\n\n\n\n\n\n\nReferences\n\nGBIF.Org User. 2022. “Occurrence Download.” The Global Biodiversity Information Facility. https://doi.org/10.15468/DL.CQPA99."
  },
  {
    "objectID": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html",
    "href": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html",
    "title": "Google’s Business Intelligence Certificate Overview",
    "section": "",
    "text": "Business intelligence aims to leverage data to generate actionable insights that improve business outcomes by way of increasing organizational data maturity. BI us multifaceted combining data analytics, software development, project management and business analysis to inform strategic, operational, and predictive business decisions.\nSkills I acquired from [Google’s Business Intelligence Certificate](https://www.coursera.org/professional-certificates/google-business-intelligence#courses) include:\n\nData engineering: Acquired deeper insights into numerous database types, design schema, and performance optimization. Used Google Cloud Platform and Big Query.\nAnalytics engineering: Developed robust ETL data pipelines for real-time analytics using Google DataFlow, SQL, and Python.\nIntelligence reporting: Mastered automated report and dashboard creation, utilizing Tableau, Python’s streamlit package, as well as R’s shiny.\nIntelligence life cycle management: Learned stakeholder engagement, information needs assessment, and problem-solving through design thinking principles.\n\nThe certificate has increased my technical capacity as well as my business acumen and stakeholder management capabilities. It has empowered me with the knowledge and skills to advance my organization’s goals with a higher level of professionalism and sophistication, further positioning us to elevate data-driven initiatives."
  },
  {
    "objectID": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#bi-stakeholders",
    "href": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#bi-stakeholders",
    "title": "Google’s Business Intelligence Certificate Overview",
    "section": "BI Stakeholders",
    "text": "BI Stakeholders\nIt can be helpful to think about the various categories of stakeholders in a BI project to better understand and articulate their needs.\nProject Sponsors are ultimately responsible and accountable for the project and often establish criteria for success.\nApplication Developers generate data and consume data and are important to work with to ensure useful data streams are identified.\nSystems analysts design and implement information systems, often back end platforms, and have a big picture in mind for how information needs to move throughout the business.\nBusiness stakeholders including executives, customer service facing employees, data science teams made up of analysts and engineers will have different requirements for information\nWhile it is important to meet with key stakeholders individually and even observe teams in action to understand the what, how and why of what they do, holding stakeholder alignment workshops is a great strategy to ensure stakeholder requirements and project requirements are aligned within the business."
  },
  {
    "objectID": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#bi-outputs-and-processes",
    "href": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#bi-outputs-and-processes",
    "title": "Google’s Business Intelligence Certificate Overview",
    "section": "BI Outputs and Processes",
    "text": "BI Outputs and Processes\nMuch of BI is about bringing various sources of data together in a way that allows them to be integrated and analyzed. The end goal is often a suite of metrics to understand what is happening in the market, in the business, or in your organization in real time. Distilling down a deluge of information into informative visualizations is the bread and butter of BI. To do that BI professionals often use ‘Extract, Transform, Load’ data pipelines that feed data into a suite of visualizations such as a dashboard, or reproducible reports.\nThe type of metrics used will depend on if its one of the three types of dashboards: 1) Strategic; 2) Operational; 3) Analytical.\nStrategic dashboards usually include Key Performance Indicators (KPIs) and a “North Star” metric. A North Star metric is the most important measure of success, and is important for organizations to define because it can align the business and the employees. Strategic dashboards track the trajectory and progress of the business over the medium to long term.\nOperational dashboards, on the other hand, offer real time insights for rapid intelligence gathering and decision making. Monitoring various aspects of a business or market which can change rapidly over the course of hours, days, or weeks. Examples might include number of sales, website up time, website traffic or customer complaints.\nAnalytical dashboards offer a deeper dive in various aspects of business data, that might allow a user to tweak some assumptions of underlying models to better predict future scenarios.\n\nBI Planning Documents\nWhatever the dashboard type, going through a standard process for gathering stakeholder requirements is essential. Three documents can ensure a BI project delivers value:\n1) Stakeholder Requirements Document is usually one-page and defines a) the business problem; b) key stakeholders names and job titles; c) primary requirements to consider the project a success.\n2) Project Requirements Document is longer and outlines why the project needs doing and why the company should invest in it.\n\nIt defines key dependencies: what data do we currently have versus what new data collections might be needed\nIdentifies major elements of the project, the team and roles within each team\nDefine a minimum viable product that should be built before embarking on more challenging aspects of the project such as new data collection procedures\nDefines the expected deliverables and success using SMART criteria\n\nIdentify key metrics with necessary technologies and processes that are already in place\nIdentify the frequency of delivery of supporting data for key metrics to identify misalignment\n\nPrioritizes features based on stakeholder requirements but also based on features with fewest dependencies\nDocuments ‘User Journeys’, ie. the current user experience and the future ideal experience\nIncludes privacy, access, legal and compliance concerns and requirements. Who needs access?\nExplicitly states any assumptions\nAccessibility: defines key considerations for accessibility; optimized for mobile or desktop? Colour blind? Large text?\nUser-support framework: How will users be empowered to contribute data sustainably and learn to use the dashboard?\nRoll-out plan: Describe scope, priorities, timeline. At what point during the roll out will measurements be made to ensure the features are performing as expected?\nKey references: Documents, policies, how to guides, links to project tracking interface\n\n3) Strategy document offers a collaborative space to align stakeholders about project deliverables.\n\nDetails of dashboard functionality, metrics, charts\nFleshes out limitations or assumptions about data\nProvides chart and dashboard mock ups\nRequest review and sign-off from key stakeholders to complete the planning phase.\n\n\n\nBI is All About Communication\nWhile it might sound cliche or rote to say communication is key, but many of us fail to understand this and suffer from misunderstandings, wasting time, and not generating the right value as a result. Intelligence is about information, how do we collect information? How do we utilize information? Communication. Communication from the technical foundation of internet protocols, APIs, data pipelines, to the human dimensions required to gather stakeholder requirements, give and receive feedback, and inform others what your newly acquired intelligence actually means. So, it’s not a matter of prioritizing communication for effective BI it about realizing that BI is communication, not just fancy tools and pretty pictures.\nTo effectively communicate in BI there are some basic considerations: communicate early and communicate often. Early communication by way of scoping stakeholder requirements, and often by way of formalized check in points. Timing and frequency is critical, but so are the methods. Asking questions is one of the best methods of communication so let’s consider how to do that most effectively:\n\nAsk effective questions\n\nAsk open-ended questions\nDon’t ask leading questions\nBe curious rather than making a point with a passive aggressive question\nDig deeper into confusing or unclear situations by asking clarifying questions\nAsk specific questions not vague ones\nAsk SMART questions: Specific, measurable, actionable, relevant, time-bound\n\n\nWhen presenting information in the form of presentation or meeting, use the who what when where why and how formula.\nKeeping track of project milestones and tracking progress towards to milestones in a project change log is a great way to stay on top of key changes that will need to be communicated. Make a plan to communicate changes from the project change log often."
  },
  {
    "objectID": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#bi-databases",
    "href": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#bi-databases",
    "title": "Google’s Business Intelligence Certificate Overview",
    "section": "BI Databases",
    "text": "BI Databases\nThere’s a lot of talk of different database types within the field of business intelligence and analytics. I’ll try to clarify some of the more common types that I found useful to understand coming from a background in biology and scientific data where the relational database is the main workhorse. What’s the difference between data lakes, data marts, data warehouses, graph databases, and more recently vector databases?\nFirst, let’s start with the difference between a database and a data warehouse.\nDatabase: Simply put a database is a collection of data stored in a computer system. They can be optimized for many purposes, but a traditional database optimized to minimize duplication, reduce storage requirements, and maintain data integrity through a process of normalization. Normalization in essence is the process of breaking data into multiple tables so that no column has a repeated row of data. Tables are then joined by identifier columns with unique values that match other tables referred to as keys. In this way each table is related some how to others, hence the term relational database. While very efficient and logical, the design of a databases structure (schema) is defined early in the design process and remains rather rigid and difficult to change later which is a significant drawback to the typical relational database. Typical examples of relational databases include PostgreSQL, MySQL, SQLite, MongoDB, and Microsoft Access.\nData warehouses: Are a type of database that consolidate data from multiple source systems aimed at ensuring consistency, accuracy, and efficient queries and and analysis. They often rely on denormalized designs such as star or snowflake schema, which facilitate faster query performances because the data are organized around important business facts or metrics and fewer tables may need to be joined. Examples of data warehouses include Google Big Query, Amazon Redshift, and Snowflake.\nOne of the key differences between a data warehouse and a relational database is that data warehouses are optimized for column-based operations which allows data to be spread across multiple servers and can increase query performance. Databases on the other hand are optimized for row-based operations, meaning that an entire row of a database has to be retrieved and scaling has to be done horizontally by increasing the capacity in a single server.\nData Lakes allow you store both structured and unstructured (videos, images etc) data at any scale. Data can be stored raw, ie in it’s original format, without having to fit into a schema thereby offering great flexibility. They are flexible and low cost but can make it difficult to access useful information can become chaotic without adequate governance and queries can be very slow due to a lack of structure.\nData Marts are subject specific databases that are optimized for specific business users. Unlike data lakes, they are structured and contain only relevant data for specific uses.\nOLTP (online transaction processing) databases are optimized processing and managing transnational data with an emphasis on fast query processing and maintaining integrity in high contention (multi-access) and high concurrency environments. Often organized in a normalized relational model. MySQL, Oracle and Microsoft SQL Server are all common examples OLTP databases.\nOLAP (online analytical processing) databases are designed for complex queries and calculations usually for the purpose of real time analytics. Often organized as a de-normalized star or snowflake schema or in columnar databases for faster query performance. Generally concurrency and contention is not an issue because a single team or user are utilizing the databases at a time. SAP BW, IBM Cognos Analytics and Microsoft Analysis Services are popular commercial OLAP databases.\nGraph databases are a huge topic and likely warrant their own blog post. There’s a lot of interest and perhaps hype surrounding graph databases. Briefly graph databases are designed to store data in the form graphs, which is the connection of nodes via edges. Graph databases are schema-less meaning the structure of the graph can be extended naturally as new relationships or nodes are needed. This offers a lot of flexibility compared to traditional relational databases. However, there’s a notion of universal applicability that graph databases can solve all relational problems, which is not true. They are often marketed as a panacea for big data complexities but scalability can still be an issue. In reality graph databases are extremely effective for niche applications and problems that are intrinsically based on relationships. However, implementing a graph database and migrating data into it can be challenging and complex.\nVector Databases have recently emerged as a critical data storage environment related to artificial intelligence. As the name implies, they store vectors often used to represent geometric objects, such as points, lines, and polygons but vectors can often represent complex entities that have been transformed into high dimensional spaces via machine learning models, such as deep neural networks. They can be useful for Content-based recommendation systems, and finding similar items based on their high dimensional feature vectors. They are also useful for natural language, processing computer vision, and anomaly detection. Lastly, vector databases are critical in storing embeddings required for large language models. They are highly scalable and enable real time applications like chat, bots, or recommendation systems to utilize large language models effectively.\nFeature-oriented databases represent a nuanced approach to database design and data storage that specifically caters to the management of important features or attributes of data. These features may be determined by subject matter experts, or extracted from text or image data using various feature extraction processes such as natural language processing or computer vision. These features are there in turn used to train machine learning models. Many AI systems depend on this type of database because the features are in a model-ready format."
  },
  {
    "objectID": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#design-thinking-and-structured-thinking",
    "href": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#design-thinking-and-structured-thinking",
    "title": "Google’s Business Intelligence Certificate Overview",
    "section": "Design Thinking and Structured Thinking",
    "text": "Design Thinking and Structured Thinking\nYou are probably familiar with structured thinking already whether you realize it or not. It’s essentially the scientific process that includes stages: problem definition, hypothesis generation, data collection, analysis and solutions. When you encounter complex problems while working on BI projects “Design Thinking” can be a very useful method to innovate solutions that are focused on user needs. Design thinking differs from structured thinking in that it takes a human-centred, iterative, explorative, and flexible approach whereas structured thinking is less collaborative, more linear, and mostly uses deductive reasoning to find the most efficient solutions.\nDesign thinking shines for:\n1) tackling uncertain and ambiguous problems when the problem-space is not well defined or when dealing with ‘wicked problems’; 2) user-centric projects; 3) Innovation; 4) Interdisciplinary challenges; and 5) Rapid prototyping and iteration.\nMany problems in environmental science are ‘wicked problems’ that don’t have a clear definition, include many stakeholders with different perspectives, complex inter-dependencies, don’t have a clear way to establish a definitive solution and have incomplete, contradictory, or changing requirements. When a purely scientific-engineering approach to solving a problem using structured thinking does not or will not work, design thinking may offer useful solutions.\n\nKey Stages of Design Thinking:\n\nEmpathize: This initial stage involves understanding the emotional experiences and needs of the end-user. Research methods may include interviews, observations, and immersion in the user environment.\nDefine: Based on the empathy stage, the problem is explicitly defined from the user’s perspective. The goal is to articulate what the user needs, not what the business needs.\nIdeate: This stage involves generating a wide variety of potential solutions through brainstorming and other ideation techniques. The focus is on volume and diversity of ideas.\nPrototype: Ideas are turned into low-fidelity prototypes, which could range from sketches to physical models. The aim is to visualize potential solutions and prepare them for testing.\nTest: The prototype is presented to end-users, and their interactions and feedback are closely observed. This data is then used to refine the solution.\n\n\n\nWhen to Apply Structured Thinking\n\nWell-Defined Problems: When you have a specific, concrete problem to solve and the criteria for success are clear.\nData-Driven Decisions: When solutions can be evaluated empirically or quantitatively.\nLogical Complexity: When the problem can be decomposed into smaller problems that require systematic analysis.\nOperational Efficiency: When the main goal is to improve the efficiency of an existing process or system.\nRisk Mitigation: When it’s critical to make decisions that minimize risk and error."
  },
  {
    "objectID": "posts/2023-05-30-Mobilizing Salmon Data/index.html",
    "href": "posts/2023-05-30-Mobilizing Salmon Data/index.html",
    "title": "How do we mobilize salmon data?",
    "section": "",
    "text": "Last updated: 2023-06-29"
  },
  {
    "objectID": "posts/2023-05-30-Mobilizing Salmon Data/index.html#whos-already-publishing-salmon-data",
    "href": "posts/2023-05-30-Mobilizing Salmon Data/index.html#whos-already-publishing-salmon-data",
    "title": "How do we mobilize salmon data?",
    "section": "Who’s already publishing salmon data?",
    "text": "Who’s already publishing salmon data?\nLet’s look through the datasets registered with DataCite that have the word salmon somewhere in their DOI’s metadata. To do this I will use DataCite’s REST API and the R package rdatacite which provides convenient functions for making API calls.\n\n\nCode\nlibrary(rdatacite)\nlibrary(tidyverse)\nlibrary(plotly)\n# Initialize lists to store dataframes and metadata\nall_datasets &lt;- list()\nall_meta &lt;- list()\n\n# Fetch initial page\ntt_salmon_datasets &lt;- dc_dois(query = \"salmon\")\n\ntt_providers &lt;- tt_salmon_datasets[[\"meta\"]][[\"providers\"]]\n# tt_affiliations &lt;- tt_salmon_datasets[[\"meta\"]][[\"affiliations\"]][[\"title\"]]\n\nplotly::ggplotly(ggplot2::ggplot(tt_providers, aes(x = reorder(title, count), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(x = \"Publisher\", y = \"Count\") +\n  theme_minimal() +\n  ggtitle(\"Top Ten Publishers of Salmon Datasets\")) \n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\ntt_affiliations &lt;- tt_salmon_datasets[[\"meta\"]][[\"affiliations\"]]\n\nggplot(tt_affiliations, aes(x = reorder(title, count), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(x = \"Institutions\", y = \"Count\") +\n  theme_minimal() +\n  ggtitle(\"Top Ten Institutions Providing Salmon Datasets\")\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\nyears_published &lt;- tt_salmon_datasets[[\"meta\"]][[\"published\"]]\n\nggplot(years_published, ggplot2::aes(x = id, y = count))+\n                   geom_bar(stat = \"identity\", fill = \"steelblue\")+\n                   labs(x = \"Year\", y = \"Count\") +\n                   coord_flip() +\n                   theme_minimal() +\n                   ggtitle(\"Growth of salmon datasets published with a DOI\")"
  },
  {
    "objectID": "posts/2023-06-23-Learning Python as an R User/index.html",
    "href": "posts/2023-06-23-Learning Python as an R User/index.html",
    "title": "Learning Python as an R User",
    "section": "",
    "text": "One of the main differences you’ll notice right away between Python and R is how difficult can be to install Python and get to the point where you’re writing code. Coming from R, it’s pretty straightforward and clear that you need to install the R language from the Comprehensive R Archive Network (CRAN), and then install R Studio and you’re basically off to the races. With Python, however, there’s a few different options. I managed to get it installed and running in VS Code using pyenv and poetry, but I needed a lot of help from a senior engineer to get it to actually work!\nAll these difficulties led me to using Python in Google Colaboratory for now. Using Colab abstracts away all the installation, environment and package management challenges, but with some trade-offs down the line if you’re interested in developing applications or using the same versions of packages and Python for reproducibility purposes. For now, it’s great to just get started understanding how the language works without worrying about all the other stuff.\n\n\nBoth languages are popular for data analysis but their distinct origin stories have resulted in some broad differences. R was developed by statisticians for statisticians as an evolution of the S programming language developed at Bell Labs by John Chambers and others in 1976. S had a strong focus on creating an interactive environment to make data analysis easier. As a result of this fundamental design decision, most experienced programmers find programming in R weird and confusing. R was released by Ross Lhaka and Robert Gentleman in 1996 as ‘free software’, as opposed to the commercial version of S, allowing the growth of language by a vibrant community. R’s strengths lay in the availability of numerous packages for statistical analysis, the ability to make ‘publication ready’ graphics, and the inclusive community that can support new programmers.\nVersion 1 of Python, on the other hand, was released in 1994, a couple years before the first version of R but almost 20 years later than S. An early focus for Python was using clean syntax, a feature that has made the language easier to learn and read compared to R. Although, the development of the pipe operator and the tidyverse in R has improved the readability substantially. But I digress. Python was built with simplicity in mind including principles such as ‘simple is better than complex’, ‘There should be one–and preferably only one–obvious way to do it’, and ‘flat is better than nested’. Those are three principles that really resonate with me as an R user given than there always seems to be a million ways to do the same thing in R and I always end up having to work with nested lists which are a pain to navigate and flatten!\nFrom my perspective, I would say that R is used more than Python in academia, especially in the fields of statistics and ecology. Python has a broader user-base, gaining traction in the ocean and atmospheric sciences and has without a doubt been more heavily used in the fields of machine learning and artificial intelligence. Python also has a reputation for having better support for web integration and deployment, though R has made strides in this space with the likes of the shiny package.\nBut, this blog post isn’t about which language is better or worse, or which you should or shouldn’t use. In practice it’s getting easier to use both languages within the same project thanks to packages like reticulate. Knowing the basics of both languages is nice if you’re working in the field of data science and will make you a better programmer, but I would argue it’s better to be an expert in one or the other rather than a novice or intermediate in both!\n\n\n\nNow, I’m going to get into the weeds a bit in terms of some of the technical differences between R and Python. Not really knowing any other programming language (apart from some C++ in high school), I was actually more surprised by the commonalities of the languages than the differences. I was expecting Python to feel very different and scary, but it turns out that quite a few of the programming paradigms in R translate well to Python.\nSo, fear Python not my useR friends.\n\n\nIn R we have:\n\nnumeric Includes integer (no decimal values) and double (has decimal values) numbers\nlogical TRUE or FALSE\ncharacter Strings of letters; always surrounded by quotes\nfactor Categorical variables. Ordered or un-ordered. Stored in memory as integers and names with character values.\ncomplex and raw also exist in R but are rarely used\n\nIn Python we have:\n\nnumeric Very similar to R and includes integers as well as decimal values called a float in Python.\nboolean TRUE or FALSE\nstring character strings declared using quotes\nnoneType A special type representing absence of a value or a null value\n\nThe main difference in R and Python in terms of Data Types is the lack of factors in Python and the presence of a noneType. However, using the pandas package, a categorical data type is introduced in Python.\n\n\n\nIn R we have:\n\nvector Can either be lists made up of different data types or atomic made up of the same data types\nmatrix A two dimensional data structure with elements of all the same class.\narray Can store data in any number of dimensions\ndataframe Typical spreadsheet style data table that is a list of atomic vectors which serve as columns. All columns must have the same number of rows and a column can only one contain one data type, though different columns can be different types (numeric, factor, etc).\n\nIn Python we have:\n\nlists Similar to R’s vectors. The are ordered collections of items which are mutable (can be changed) and can contain a mix of types (int, float, str, etc.). The can be indexed, concatenated and sliced.\ntuples Similar to lists but are immutable (can’t be changed after creation)\nsets Un-ordered collections of items.\ndictionaries Store key and value pairs. Similar to named lists in R where elements can be accessed using names rather than by their position.\ndataframes Python doesn’t natively support dataframes but the pandas library provides this structure and works similarly to dataframes in R.\narrays The numpy library provides a data structure that works like an array in R. These are often used for mathematical operations when efficiency is required.\n\n\n\n\nR starts counting at 1. Python starts counting at 0. That’s hard to get used to. So for example, to access the item at the start of a list in Python you would write my_list[0] whereas in R you would write my_list[1].\n\n\n\nIn R, sub setting vectors and dataframes can be accomplished many ways: $, subset(), [[, [ and the with functions to access elements of a data frame. It can be a bit confusing to know which one to use or read code that switches between the different methods.\nIn Python, the primary method of subsetting is using single square brackets: []. This works on strings, lists, dictionaries and tuples. You can get single elements back by indexing just one position, or get what Python calls a “slice” back by using a range. For example, my_string[1:3] returns a slice of the 2nd, 3rd, and 4th elements of my_string (remember zero indexing).\n\n\n\nIn R there are two assignment operators: = and &lt;- The equal sign is generally used in parameter definitions inside a function call ie my_func(a = 1) whereas the &lt;- assignment operator can be used in most (if not all) other instances.\nIn Python there’s just the good old equals sign for assignment =\n\n\n\nScoping refers to the visibility of a variable to other parts of code. Both R and Python use “lexical scoping”, but with a few key differences.\nIn R, the scope of a variable is determined by the environment in which it was created. R will first look into the current environment for that variable and if it’s not found it will continue to ‘go up a level’ to enclosing environments until it either finds the variable of the variable is not found in the global environment and eventually the system environment.\nIn Python, it’s fairly similar but there are a few extra scoping features to be aware of: you can chose to modify a global variable from within a function using the global keyword. For example:\nx = 10  # Here x is a global variable\n\n# define a function that modifys the global variable x\ndef modify_global():\n    global x  # We declare that we want the global x\n    x = 20  # This will change the global x\n\nprint(x)  # Output: 10\nmodify_global()\nprint(x)  # Output: 20\nIf you’re into writing nested functions, you can also chose which outer environment you would like to change using the nonlocal keyword so that you modify the variable in the immediately enclosing environment rather than the local environment or the global environment. For example:\ndef outer():\n    x = 10  # Here x is a variable local to the function outer, but non-local to the function inner\n    def inner():\n        nonlocal x  # We declare that we want the nonlocal x\n        x = 20  # This will change the nonlocal x\n\n    print(x)  # Output: 10\n    inner()\n    print(x)  # Output: 20\n\nouter()\nIn this example, the nonlocal keyword is used in the nested function inner to indicate that x refers to the x in the immediately enclosing scope, which is the function outer. Without the nonlocal keyword, x would be treated as a local variable within inner, and the assignment x = 20 would not affect the x in outer. This actually seems like a recipe for really confusing code :|\n\n\n\nR’s approach to OOP is more complex because it has not one, but five different OOP systems: S3, S4, RC, R6 and now R7.\nS3 and S4 are more function-oriented - methods belong to functions, not classes, unlike Python where methods belong to classes. RC is more like typical OOP in this respect in that methods belong to classes, but is rarely used in R. R6 is a package rather than part of base R, is similar to RC and was primarily developed for use with the Shiny package by Posit (R Studio). R7 was released quite recently and aims to consolidate the good parts of the various systems and simplify everyone’s lives. TBD if that happens ;)\nThe various implementations of OOP in R make it confusing. I admit I try to avoid getting to deep into the differences here, but you can end up in a world of confusion when these systems get intertwined.\nPython’s OOP:\nPython supports OOP with a simple, easy-to-understand syntax and structure. The key components of Python’s OOP are classes, objects, and methods. Critically, there is only one OOP system in Python!\n\nClasses are like blueprints for creating objects (a particular data structure). They define the properties (also called attributes) that the object should have and the methods (functions) that the object can perform.\nObjects are instances of a class, which can have attributes and methods.\nMethods are functions defined within a class, used to define the behaviours of the objects."
  },
  {
    "objectID": "posts/2023-06-23-Learning Python as an R User/index.html#general-differences-between-r-and-python",
    "href": "posts/2023-06-23-Learning Python as an R User/index.html#general-differences-between-r-and-python",
    "title": "Learning Python as an R User",
    "section": "",
    "text": "Both languages are popular for data analysis but their distinct origin stories have resulted in some broad differences. R was developed by statisticians for statisticians as an evolution of the S programming language developed at Bell Labs by John Chambers and others in 1976. S had a strong focus on creating an interactive environment to make data analysis easier. As a result of this fundamental design decision, most experienced programmers find programming in R weird and confusing. R was released by Ross Lhaka and Robert Gentleman in 1996 as ‘free software’, as opposed to the commercial version of S, allowing the growth of language by a vibrant community. R’s strengths lay in the availability of numerous packages for statistical analysis, the ability to make ‘publication ready’ graphics, and the inclusive community that can support new programmers.\nVersion 1 of Python, on the other hand, was released in 1994, a couple years before the first version of R but almost 20 years later than S. An early focus for Python was using clean syntax, a feature that has made the language easier to learn and read compared to R. Although, the development of the pipe operator and the tidyverse in R has improved the readability substantially. But I digress. Python was built with simplicity in mind including principles such as ‘simple is better than complex’, ‘There should be one–and preferably only one–obvious way to do it’, and ‘flat is better than nested’. Those are three principles that really resonate with me as an R user given than there always seems to be a million ways to do the same thing in R and I always end up having to work with nested lists which are a pain to navigate and flatten!\nFrom my perspective, I would say that R is used more than Python in academia, especially in the fields of statistics and ecology. Python has a broader user-base, gaining traction in the ocean and atmospheric sciences and has without a doubt been more heavily used in the fields of machine learning and artificial intelligence. Python also has a reputation for having better support for web integration and deployment, though R has made strides in this space with the likes of the shiny package.\nBut, this blog post isn’t about which language is better or worse, or which you should or shouldn’t use. In practice it’s getting easier to use both languages within the same project thanks to packages like reticulate. Knowing the basics of both languages is nice if you’re working in the field of data science and will make you a better programmer, but I would argue it’s better to be an expert in one or the other rather than a novice or intermediate in both!"
  },
  {
    "objectID": "posts/2023-06-23-Learning Python as an R User/index.html#technical-differences",
    "href": "posts/2023-06-23-Learning Python as an R User/index.html#technical-differences",
    "title": "Learning Python as an R User",
    "section": "",
    "text": "Now, I’m going to get into the weeds a bit in terms of some of the technical differences between R and Python. Not really knowing any other programming language (apart from some C++ in high school), I was actually more surprised by the commonalities of the languages than the differences. I was expecting Python to feel very different and scary, but it turns out that quite a few of the programming paradigms in R translate well to Python.\nSo, fear Python not my useR friends.\n\n\nIn R we have:\n\nnumeric Includes integer (no decimal values) and double (has decimal values) numbers\nlogical TRUE or FALSE\ncharacter Strings of letters; always surrounded by quotes\nfactor Categorical variables. Ordered or un-ordered. Stored in memory as integers and names with character values.\ncomplex and raw also exist in R but are rarely used\n\nIn Python we have:\n\nnumeric Very similar to R and includes integers as well as decimal values called a float in Python.\nboolean TRUE or FALSE\nstring character strings declared using quotes\nnoneType A special type representing absence of a value or a null value\n\nThe main difference in R and Python in terms of Data Types is the lack of factors in Python and the presence of a noneType. However, using the pandas package, a categorical data type is introduced in Python.\n\n\n\nIn R we have:\n\nvector Can either be lists made up of different data types or atomic made up of the same data types\nmatrix A two dimensional data structure with elements of all the same class.\narray Can store data in any number of dimensions\ndataframe Typical spreadsheet style data table that is a list of atomic vectors which serve as columns. All columns must have the same number of rows and a column can only one contain one data type, though different columns can be different types (numeric, factor, etc).\n\nIn Python we have:\n\nlists Similar to R’s vectors. The are ordered collections of items which are mutable (can be changed) and can contain a mix of types (int, float, str, etc.). The can be indexed, concatenated and sliced.\ntuples Similar to lists but are immutable (can’t be changed after creation)\nsets Un-ordered collections of items.\ndictionaries Store key and value pairs. Similar to named lists in R where elements can be accessed using names rather than by their position.\ndataframes Python doesn’t natively support dataframes but the pandas library provides this structure and works similarly to dataframes in R.\narrays The numpy library provides a data structure that works like an array in R. These are often used for mathematical operations when efficiency is required.\n\n\n\n\nR starts counting at 1. Python starts counting at 0. That’s hard to get used to. So for example, to access the item at the start of a list in Python you would write my_list[0] whereas in R you would write my_list[1].\n\n\n\nIn R, sub setting vectors and dataframes can be accomplished many ways: $, subset(), [[, [ and the with functions to access elements of a data frame. It can be a bit confusing to know which one to use or read code that switches between the different methods.\nIn Python, the primary method of subsetting is using single square brackets: []. This works on strings, lists, dictionaries and tuples. You can get single elements back by indexing just one position, or get what Python calls a “slice” back by using a range. For example, my_string[1:3] returns a slice of the 2nd, 3rd, and 4th elements of my_string (remember zero indexing).\n\n\n\nIn R there are two assignment operators: = and &lt;- The equal sign is generally used in parameter definitions inside a function call ie my_func(a = 1) whereas the &lt;- assignment operator can be used in most (if not all) other instances.\nIn Python there’s just the good old equals sign for assignment =\n\n\n\nScoping refers to the visibility of a variable to other parts of code. Both R and Python use “lexical scoping”, but with a few key differences.\nIn R, the scope of a variable is determined by the environment in which it was created. R will first look into the current environment for that variable and if it’s not found it will continue to ‘go up a level’ to enclosing environments until it either finds the variable of the variable is not found in the global environment and eventually the system environment.\nIn Python, it’s fairly similar but there are a few extra scoping features to be aware of: you can chose to modify a global variable from within a function using the global keyword. For example:\nx = 10  # Here x is a global variable\n\n# define a function that modifys the global variable x\ndef modify_global():\n    global x  # We declare that we want the global x\n    x = 20  # This will change the global x\n\nprint(x)  # Output: 10\nmodify_global()\nprint(x)  # Output: 20\nIf you’re into writing nested functions, you can also chose which outer environment you would like to change using the nonlocal keyword so that you modify the variable in the immediately enclosing environment rather than the local environment or the global environment. For example:\ndef outer():\n    x = 10  # Here x is a variable local to the function outer, but non-local to the function inner\n    def inner():\n        nonlocal x  # We declare that we want the nonlocal x\n        x = 20  # This will change the nonlocal x\n\n    print(x)  # Output: 10\n    inner()\n    print(x)  # Output: 20\n\nouter()\nIn this example, the nonlocal keyword is used in the nested function inner to indicate that x refers to the x in the immediately enclosing scope, which is the function outer. Without the nonlocal keyword, x would be treated as a local variable within inner, and the assignment x = 20 would not affect the x in outer. This actually seems like a recipe for really confusing code :|\n\n\n\nR’s approach to OOP is more complex because it has not one, but five different OOP systems: S3, S4, RC, R6 and now R7.\nS3 and S4 are more function-oriented - methods belong to functions, not classes, unlike Python where methods belong to classes. RC is more like typical OOP in this respect in that methods belong to classes, but is rarely used in R. R6 is a package rather than part of base R, is similar to RC and was primarily developed for use with the Shiny package by Posit (R Studio). R7 was released quite recently and aims to consolidate the good parts of the various systems and simplify everyone’s lives. TBD if that happens ;)\nThe various implementations of OOP in R make it confusing. I admit I try to avoid getting to deep into the differences here, but you can end up in a world of confusion when these systems get intertwined.\nPython’s OOP:\nPython supports OOP with a simple, easy-to-understand syntax and structure. The key components of Python’s OOP are classes, objects, and methods. Critically, there is only one OOP system in Python!\n\nClasses are like blueprints for creating objects (a particular data structure). They define the properties (also called attributes) that the object should have and the methods (functions) that the object can perform.\nObjects are instances of a class, which can have attributes and methods.\nMethods are functions defined within a class, used to define the behaviours of the objects."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decode the Overload",
    "section": "",
    "text": "Google’s Business Intelligence Certificate Overview\n\n\n\n\n\n\n\nBusiness Intelligence\n\n\nGoogle\n\n\nProfessional Development\n\n\n\n\nBusiness Intelligence Methods for Scientific Data Mobilization\n\n\n\n\n\n\nSep 5, 2023\n\n\nBrett Johnson\n\n\n\n\n\n\n  \n\n\n\n\nLearning Python as an R User\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nData Science\n\n\n\n\nPython versus R? Nah Python + R\n\n\n\n\n\n\nJun 23, 2023\n\n\nBrett Johnson\n\n\n\n\n\n\n  \n\n\n\n\nHow Does the Global Biodiversity Information System Count Citations from the International Year of the Salmon?\n\n\n\n\n\n\n\nBiodiversity\n\n\nGBIF\n\n\nDatacite\n\n\nInternational Year of the Salmon\n\n\n\n\nHow to use the DataCite REST API to understand how GBIF counts citations\n\n\n\n\n\n\nJun 20, 2023\n\n\nBrett Johnson\n\n\n\n\n\n\n  \n\n\n\n\nHow do we mobilize salmon data?\n\n\n\n\n\n\n\nSalmon\n\n\necology\n\n\n\n\nCommunicate, assemble and coordinate extant data systems\n\n\n\n\n\n\nMay 30, 2023\n\n\nBrett Johnnson\n\n\n\n\n\n\nNo matching items"
  }
]