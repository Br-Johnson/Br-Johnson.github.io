[
  {
    "objectID": "posts/2023-05-30-Mobilizing Salmon Data/index.html",
    "href": "posts/2023-05-30-Mobilizing Salmon Data/index.html",
    "title": "How do we mobilize salmon data?",
    "section": "",
    "text": "There exists a mountain of data about salmon, their environment, and abundance yet we struggle to understand why or predict when certain populations of salmon decline. Part of the challenge is that salmon have a complicated life history where they are exposed to multiple environments throughout their lives. Some parts of their lives are easier to observe than others and some environments are more complex than others. The challenge is to figure out what observations have the most predictive power in each life phase, and then put all those data together to glean a complete and predictive history of the conditions encountered by specific salmon stocks.\nHowever, because salmon cross arbitrary municipal, provincial, and national borders we lack a coordinated approach to aggregating data. Data often are collected using bespoke standards, stored locally, and not shared. Data management is not a trivial task. Therefore, any coordinated approach needs to be lightweight and flexible such that the barrier to adherence is surmountable by individual biologists and fall in line with the basic tenets of FAIR data.\nIndeed, the key to mobilizing salmon data doesn’t lie in creating new technologies. Rather, it rests on effectively harnessing and integrating existing ones, ensuring they are user-friendly, supporting their broad adoption, and raising awareness about their use. It’s essential to remember that adhering to practical principles and modern best-practices that solve immediate problems will propel us further faster than being distracted by the allure of novel technologies or flashy platforms. Success hinges on assisting agencies to meet their data publishing mandates. To achieve this, we must address challenges faced by individual biologists and data managers one at a time, providing solutions rooted in practicality and principles that are underpinned by well-adopted global best-practices, and aligning these with existing national mandates. By sticking to the fundamentals outlined below, the salmon research community will be well-positioned to effectively manage, integrate, and harmonize the vast amount of data being produced to understand, predict and manage changes to salmon ecoystems."
  },
  {
    "objectID": "posts/2023-05-30-Mobilizing Salmon Data/index.html#proof-of-concept",
    "href": "posts/2023-05-30-Mobilizing Salmon Data/index.html#proof-of-concept",
    "title": "How do we mobilize salmon data?",
    "section": "Proof of Concept",
    "text": "Proof of Concept\nLet’s look through DOIs registered with DataCite that have the word salmon somewhere in the digital objects metadata. To do this I will use DataCite’s REST API and the R package rdatacite which provides convenient functions for making API calls.\n\n\nCode\nlibrary(rdatacite)\nlibrary(tidyverse)\nlibrary(plotly)\n# Initialize lists to store dataframes and metadata\nall_datasets &lt;- list()\nall_meta &lt;- list()\n\n# Fetch initial page\ntt_salmon_datasets &lt;- dc_dois(query = \"salmon\")\n\ntt_providers &lt;- tt_salmon_datasets[[\"meta\"]][[\"providers\"]]\ntt_affiliations &lt;- tt_salmon_datasets[[\"meta\"]][[\"affiliations\"]][[\"title\"]]\n\nplotly::ggplotly(ggplot2::ggplot(tt_providers, aes(x = reorder(title, count), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(x = \"Publisher\", y = \"Count\") +\n  theme_minimal() +\n  ggtitle(\"Top Ten Publishers of Salmon Datasets\")) \n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\ntt_affiliations &lt;- tt_salmon_datasets[[\"meta\"]][[\"affiliations\"]]\n\nggplot(tt_affiliations, aes(x = reorder(title, count), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(x = \"Publisher\", y = \"Count\") +\n  theme_minimal() +\n  ggtitle(\"Top Ten Institutions Providing Salmon Datasets\")\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\nyears_published &lt;- tt_salmon_datasets[[\"meta\"]][[\"published\"]]\n\nggplot(years_published, ggplot2::aes(x = id, y = count))+\n                   geom_bar(stat = \"identity\", fill = \"steelblue\")+\n                   labs(x = \"Year\", y = \"Count\") +\n                   coord_flip() +\n                   theme_minimal() +\n                   ggtitle(\"Growth of salmon datasets published with a DOI\")"
  },
  {
    "objectID": "posts/2023-06-22-How Does GBIF Count Citations?/gbif.html",
    "href": "posts/2023-06-22-How Does GBIF Count Citations?/gbif.html",
    "title": "How Does the Global Biodiversity Information System Count Citations?",
    "section": "",
    "text": "Introduction\nIf you publish your data to the Global Biodiversity Information Facility (GBIF), you may notice that your dataset is being cited by papers that seemingly have nothing to do with the context of your original project that produced the dataset.\nI had a colleague involved with the International Year of the Salmon (IYS) ask why IYS datasets such as one dataset about zooplankton was referenced in a paper that seemed to be about terrestrial species [@garcía-roselló2023].\nIf you look in the references of the Garcia-Rosello et al 2023 paper you will see two links to GBIF Occurrence Downloads. If you click on the DOI for GBIF.org, 2022b (https://doi.org/10.15468/dl.cqpa99), you will reach a landing page for an Occurrence Download from GBIF from a specific query. You can see the specific query filters that were used to generate the dataset.\nA total of 18,426 datasets that meet the query filter. The filter included: has CC BY 4.0 licence, Basis of record must be either Human observation, Observation or Preserved Specimen, Has coordinate is true, … Scientific name includes Animalia). As it turns out, this is a very broad filter basically asking for all the animal occurrences on GBIF datasets that have the right license and metadata and the dataset about zooplankton met all those criteria. If you wanted to see all the datasets included in the new dataset you can download a list of the involved datasets using the “Download as TSV” link from the GBIF Occurrence Dataset landing page for this big dataset and search for International Year of the Salmon to see which datasets are included.\n\nSo the dataset that resulted from this specific query includes occurrences meeting the query filter parameters from 18,426 other GBIF datasets receives a Digital Object Identifier. Each of those underlying datasets also has a digital object identifier and each of those DOIs is referenced in the new dataset’s DOI metadata as a ‘relatedIdentifier’.\n\nSo, each time the overarching dataset is cited, the citation count for each of the 18,426 datasets increases by one as well. Pretty cool, huh!?\n\nI can be surprising how the International Year of the Salmon data are re-used way outside the context of their collection, which is a major advantage of using the GBIF and publishing data using international standards.\nHere’s how you can access the metadata about the overarching DOI as well to see what kind of datasets it included."
  },
  {
    "objectID": "posts/2023-06-23-Learning Python as an R User/index.html",
    "href": "posts/2023-06-23-Learning Python as an R User/index.html",
    "title": "Learning Python as an R User",
    "section": "",
    "text": "One of the main differences you’ll notice right away between Python and R is how difficult can be to install Python and get to the point where you’re writing code. Coming from R, it’s pretty straightforward and clear that you need to install the R language from the Comprehensive R Archive Network (CRAN), and then install R Studio and you’re basically off to the races. With Python, however, there’s a few different options. I managed to get it installed and running in VS Code using pyenv and poetry, but I needed a lot of help from a senior engineer to get it to actually work!\nAll these difficulties led me to using Python in Google Colaboratory for now. Using Colab abstracts away all the installation, environment and package management challenges, but with some trade-offs down the line if you’re interested in developing applications or using the same versions of packages and Python for reproducibility purposes. For now, it’s great to just get started understanding how the language works without worrying about all the other stuff.\n\n\nBoth languages are popular for data analysis but their distinct origin stories have resulted in some broad differences. R was developed by statisticians for statisticians as an evolution of the S programming language developed at Bell Labs by John Chambers and others in 1976. S had a strong focus on creating an interactive environment to make data analysis easier. As a result of this fundamental design decision, most experienced programmers find programming in R weird and confusing. R was released by Ross Lhaka and Robert Gentleman in 1996 as ‘free software’, as opposed to the commercial version of S, allowing the growth of language by a vibrant community. R’s strengths lay in the availability of numerous packages for statistical analysis, the ability to make ‘publication ready’ graphics, and the inclusive community that can support new programmers.\nVersion 1 of Python, on the other hand, was released in 1994, a couple years before the first version of R but almost 20 years later than S. An early focus for Python was using clean syntax, a feature that has made the language easier to learn and read compared to R. Although, the development of the pipe operator and the tidyverse in R has improved the readability substantially. But I digress. Python was built with simplicity in mind including principles such as ‘simple is better than complex’, ‘There should be one–and preferably only one–obvious way to do it’, and ‘flat is better than nested’. Those are three principles that really resonate with me as an R user given than there always seems to be a million ways to do the same thing in R and I always end up having to work with nested lists which are a pain to navigate and flatten!\nFrom my perspective, I would say that R is used more than Python in academia, especially in the fields of statistics and ecology. Python has a broader user-base, gaining traction in the ocean and atmospheric sciences and has without a doubt been more heavily used in the fields of machine learning and artificial intelligence. Python also has a reputation for having better support for web integration and deployment, though R has made strides in this space with the likes of the shiny package.\nBut, this blog post isn’t about which language is better or worse, or which you should or shouldn’t use. In practice it’s getting easier to use both languages within the same project thanks to packages like reticulate. Knowing the basics of both languages is nice if you’re working in the field of data science and will make you a better programmer, but I would argue it’s better to be an expert in one or the other rather than a novice or intermediate in both!\n\n\n\nNow, I’m going to get into the weeds a bit in terms of some of the technical differences between R and Python. Not really knowing any other programming language (apart from some C++ in high school), I was actually more surprised by the commonalities of the languages than the differences. I was expecting Python to feel very different and scary, but it turns out that quite a few of the programming paradigms in R translate well to Python.\nSo, fear Python not my useR friends.\n\n\nIn R we have:\n\nnumeric Includes integer (no decimal values) and double (has decimal values) numbers\nlogical TRUE or FALSE\ncharacter Strings of letters; always surrounded by quotes\nfactor Categorical variables. Ordered or un-ordered. Stored in memory as integers and names with character values.\ncomplex and raw also exist in R but are rarely used\n\nIn Python we have:\n\nnumeric Very similar to R and includes integers as well as decimal values called a float in Python.\nboolean TRUE or FALSE\nstring character strings declared using quotes\nnoneType A special type representing absence of a value or a null value\n\nThe main difference in R and Python in terms of Data Types is the lack of factors in Python and the presence of a noneType. However, using the pandas package, a categorical data type is introduced in Python.\n\n\n\nIn R we have:\n\nvector Can either be lists made up of different data types or atomic made up of the same data types\nmatrix A two dimensional data structure with elements of all the same class.\narray Can store data in any number of dimensions\ndataframe Typical spreadsheet style data table that is a list of atomic vectors which serve as columns. All columns must have the same number of rows and a column can only one contain one data type, though different columns can be different types (numeric, factor, etc).\n\nIn Python we have:\n\nlists Similar to R’s vectors. The are ordered collections of items which are mutable (can be changed) and can contain a mix of types (int, float, str, etc.). The can be indexed, concatenated and sliced.\ntuples Similar to lists but are immutable (can’t be changed after creation)\nsets Un-ordered collections of items.\ndictionaries Store key and value pairs. Similar to named lists in R where elements can be accessed using names rather than by their position.\ndataframes Python doesn’t natively support dataframes but the pandas library provides this structure and works similarly to dataframes in R.\narrays The numpy library provides a data structure that works like an array in R. These are often used for mathematical operations when efficiency is required.\n\n\n\n\nR starts counting at 1. Python starts counting at 0. That’s hard to get used to. So for example, to access the item at the start of a list in Python you would write my_list[0] whereas in R you would write my_list[1].\n\n\n\nIn R, sub setting vectors and dataframes can be accomplished many ways: $, subset(), [[, [ and the with functions to access elements of a data frame. It can be a bit confusing to know which one to use or read code that switches between the different methods.\nIn Python, the primary method of subsetting is using single square brackets: []. This works on strings, lists, dictionaries and tuples. You can get single elements back by indexing just one position, or get what Python calls a “slice” back by using a range. For example, my_string[1:3] returns a slice of the 2nd, 3rd, and 4th elements of my_string (remember zero indexing).\n\n\n\nIn R there are two assignment operators: = and &lt;- The equal sign is generally used in parameter definitions inside a function call ie my_func(a = 1) whereas the &lt;- assignment operator can be used in most (if not all) other instances.\nIn Python there’s just the good old equals sign for assignment =\n\n\n\nScoping refers to the visibility of a variable to other parts of code. Both R and Python use “lexical scoping”, but with a few key differences.\nIn R, the scope of a variable is determined by the environment in which it was created. R will first look into the current environment for that variable and if it’s not found it will continue to ‘go up a level’ to enclosing environments until it either finds the variable of the variable is not found in the global environment and eventually the system environment.\nIn Python, it’s fairly similar but there are a few extra scoping features to be aware of: you can chose to modify a global variable from within a function using the global keyword. For example:\nx = 10  # Here x is a global variable\n\n# define a function that modifys the global variable x\ndef modify_global():\n    global x  # We declare that we want the global x\n    x = 20  # This will change the global x\n\nprint(x)  # Output: 10\nmodify_global()\nprint(x)  # Output: 20\nIf you’re into writing nested functions, you can also chose which outer environment you would like to change using the nonlocal keyword so that you modify the variable in the immediately enclosing environment rather than the local environment or the global environment. For example:\ndef outer():\n    x = 10  # Here x is a variable local to the function outer, but non-local to the function inner\n    def inner():\n        nonlocal x  # We declare that we want the nonlocal x\n        x = 20  # This will change the nonlocal x\n\n    print(x)  # Output: 10\n    inner()\n    print(x)  # Output: 20\n\nouter()\nIn this example, the nonlocal keyword is used in the nested function inner to indicate that x refers to the x in the immediately enclosing scope, which is the function outer. Without the nonlocal keyword, x would be treated as a local variable within inner, and the assignment x = 20 would not affect the x in outer. This actually seems like a recipe for really confusing code :|\n\n\n\nR’s approach to OOP is more complex because it has not one, but five different OOP systems: S3, S4, RC, R6 and now R7.\nS3 and S4 are more function-oriented - methods belong to functions, not classes, unlike Python where methods belong to classes. RC is more like typical OOP in this respect in that methods belong to classes, but is rarely used in R. R6 is a package rather than part of base R, is similar to RC and was primarily developed for use with the Shiny package by Posit (R Studio). R7 was released quite recently and aims to consolidate the good parts of the various systems and simplify everyone’s lives. TBD if that happens ;)\nThe various implementations of OOP in R make it confusing. I admit I try to avoid getting to deep into the differences here, but you can end up in a world of confusion when these systems get intertwined.\nPython’s OOP:\nPython supports OOP with a simple, easy-to-understand syntax and structure. The key components of Python’s OOP are classes, objects, and methods. Critically, there is only one OOP system in Python!\n\nClasses are like blueprints for creating objects (a particular data structure). They define the properties (also called attributes) that the object should have and the methods (functions) that the object can perform.\nObjects are instances of a class, which can have attributes and methods.\nMethods are functions defined within a class, used to define the behaviours of the objects."
  },
  {
    "objectID": "posts/2023-06-23-Learning Python as an R User/index.html#general-differences-between-r-and-python",
    "href": "posts/2023-06-23-Learning Python as an R User/index.html#general-differences-between-r-and-python",
    "title": "Learning Python as an R User",
    "section": "",
    "text": "Both languages are popular for data analysis but their distinct origin stories have resulted in some broad differences. R was developed by statisticians for statisticians as an evolution of the S programming language developed at Bell Labs by John Chambers and others in 1976. S had a strong focus on creating an interactive environment to make data analysis easier. As a result of this fundamental design decision, most experienced programmers find programming in R weird and confusing. R was released by Ross Lhaka and Robert Gentleman in 1996 as ‘free software’, as opposed to the commercial version of S, allowing the growth of language by a vibrant community. R’s strengths lay in the availability of numerous packages for statistical analysis, the ability to make ‘publication ready’ graphics, and the inclusive community that can support new programmers.\nVersion 1 of Python, on the other hand, was released in 1994, a couple years before the first version of R but almost 20 years later than S. An early focus for Python was using clean syntax, a feature that has made the language easier to learn and read compared to R. Although, the development of the pipe operator and the tidyverse in R has improved the readability substantially. But I digress. Python was built with simplicity in mind including principles such as ‘simple is better than complex’, ‘There should be one–and preferably only one–obvious way to do it’, and ‘flat is better than nested’. Those are three principles that really resonate with me as an R user given than there always seems to be a million ways to do the same thing in R and I always end up having to work with nested lists which are a pain to navigate and flatten!\nFrom my perspective, I would say that R is used more than Python in academia, especially in the fields of statistics and ecology. Python has a broader user-base, gaining traction in the ocean and atmospheric sciences and has without a doubt been more heavily used in the fields of machine learning and artificial intelligence. Python also has a reputation for having better support for web integration and deployment, though R has made strides in this space with the likes of the shiny package.\nBut, this blog post isn’t about which language is better or worse, or which you should or shouldn’t use. In practice it’s getting easier to use both languages within the same project thanks to packages like reticulate. Knowing the basics of both languages is nice if you’re working in the field of data science and will make you a better programmer, but I would argue it’s better to be an expert in one or the other rather than a novice or intermediate in both!"
  },
  {
    "objectID": "posts/2023-06-23-Learning Python as an R User/index.html#technical-differences",
    "href": "posts/2023-06-23-Learning Python as an R User/index.html#technical-differences",
    "title": "Learning Python as an R User",
    "section": "",
    "text": "Now, I’m going to get into the weeds a bit in terms of some of the technical differences between R and Python. Not really knowing any other programming language (apart from some C++ in high school), I was actually more surprised by the commonalities of the languages than the differences. I was expecting Python to feel very different and scary, but it turns out that quite a few of the programming paradigms in R translate well to Python.\nSo, fear Python not my useR friends.\n\n\nIn R we have:\n\nnumeric Includes integer (no decimal values) and double (has decimal values) numbers\nlogical TRUE or FALSE\ncharacter Strings of letters; always surrounded by quotes\nfactor Categorical variables. Ordered or un-ordered. Stored in memory as integers and names with character values.\ncomplex and raw also exist in R but are rarely used\n\nIn Python we have:\n\nnumeric Very similar to R and includes integers as well as decimal values called a float in Python.\nboolean TRUE or FALSE\nstring character strings declared using quotes\nnoneType A special type representing absence of a value or a null value\n\nThe main difference in R and Python in terms of Data Types is the lack of factors in Python and the presence of a noneType. However, using the pandas package, a categorical data type is introduced in Python.\n\n\n\nIn R we have:\n\nvector Can either be lists made up of different data types or atomic made up of the same data types\nmatrix A two dimensional data structure with elements of all the same class.\narray Can store data in any number of dimensions\ndataframe Typical spreadsheet style data table that is a list of atomic vectors which serve as columns. All columns must have the same number of rows and a column can only one contain one data type, though different columns can be different types (numeric, factor, etc).\n\nIn Python we have:\n\nlists Similar to R’s vectors. The are ordered collections of items which are mutable (can be changed) and can contain a mix of types (int, float, str, etc.). The can be indexed, concatenated and sliced.\ntuples Similar to lists but are immutable (can’t be changed after creation)\nsets Un-ordered collections of items.\ndictionaries Store key and value pairs. Similar to named lists in R where elements can be accessed using names rather than by their position.\ndataframes Python doesn’t natively support dataframes but the pandas library provides this structure and works similarly to dataframes in R.\narrays The numpy library provides a data structure that works like an array in R. These are often used for mathematical operations when efficiency is required.\n\n\n\n\nR starts counting at 1. Python starts counting at 0. That’s hard to get used to. So for example, to access the item at the start of a list in Python you would write my_list[0] whereas in R you would write my_list[1].\n\n\n\nIn R, sub setting vectors and dataframes can be accomplished many ways: $, subset(), [[, [ and the with functions to access elements of a data frame. It can be a bit confusing to know which one to use or read code that switches between the different methods.\nIn Python, the primary method of subsetting is using single square brackets: []. This works on strings, lists, dictionaries and tuples. You can get single elements back by indexing just one position, or get what Python calls a “slice” back by using a range. For example, my_string[1:3] returns a slice of the 2nd, 3rd, and 4th elements of my_string (remember zero indexing).\n\n\n\nIn R there are two assignment operators: = and &lt;- The equal sign is generally used in parameter definitions inside a function call ie my_func(a = 1) whereas the &lt;- assignment operator can be used in most (if not all) other instances.\nIn Python there’s just the good old equals sign for assignment =\n\n\n\nScoping refers to the visibility of a variable to other parts of code. Both R and Python use “lexical scoping”, but with a few key differences.\nIn R, the scope of a variable is determined by the environment in which it was created. R will first look into the current environment for that variable and if it’s not found it will continue to ‘go up a level’ to enclosing environments until it either finds the variable of the variable is not found in the global environment and eventually the system environment.\nIn Python, it’s fairly similar but there are a few extra scoping features to be aware of: you can chose to modify a global variable from within a function using the global keyword. For example:\nx = 10  # Here x is a global variable\n\n# define a function that modifys the global variable x\ndef modify_global():\n    global x  # We declare that we want the global x\n    x = 20  # This will change the global x\n\nprint(x)  # Output: 10\nmodify_global()\nprint(x)  # Output: 20\nIf you’re into writing nested functions, you can also chose which outer environment you would like to change using the nonlocal keyword so that you modify the variable in the immediately enclosing environment rather than the local environment or the global environment. For example:\ndef outer():\n    x = 10  # Here x is a variable local to the function outer, but non-local to the function inner\n    def inner():\n        nonlocal x  # We declare that we want the nonlocal x\n        x = 20  # This will change the nonlocal x\n\n    print(x)  # Output: 10\n    inner()\n    print(x)  # Output: 20\n\nouter()\nIn this example, the nonlocal keyword is used in the nested function inner to indicate that x refers to the x in the immediately enclosing scope, which is the function outer. Without the nonlocal keyword, x would be treated as a local variable within inner, and the assignment x = 20 would not affect the x in outer. This actually seems like a recipe for really confusing code :|\n\n\n\nR’s approach to OOP is more complex because it has not one, but five different OOP systems: S3, S4, RC, R6 and now R7.\nS3 and S4 are more function-oriented - methods belong to functions, not classes, unlike Python where methods belong to classes. RC is more like typical OOP in this respect in that methods belong to classes, but is rarely used in R. R6 is a package rather than part of base R, is similar to RC and was primarily developed for use with the Shiny package by Posit (R Studio). R7 was released quite recently and aims to consolidate the good parts of the various systems and simplify everyone’s lives. TBD if that happens ;)\nThe various implementations of OOP in R make it confusing. I admit I try to avoid getting to deep into the differences here, but you can end up in a world of confusion when these systems get intertwined.\nPython’s OOP:\nPython supports OOP with a simple, easy-to-understand syntax and structure. The key components of Python’s OOP are classes, objects, and methods. Critically, there is only one OOP system in Python!\n\nClasses are like blueprints for creating objects (a particular data structure). They define the properties (also called attributes) that the object should have and the methods (functions) that the object can perform.\nObjects are instances of a class, which can have attributes and methods.\nMethods are functions defined within a class, used to define the behaviours of the objects."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decode the Overload",
    "section": "",
    "text": "Learning Python as an R User\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nData Science\n\n\n\n\nPython versus R? Nah Python + R\n\n\n\n\n\n\nJun 23, 2023\n\n\nBrett Johnson\n\n\n\n\n\n\n  \n\n\n\n\nHow Does the Global Biodiversity Information System Count Citations?\n\n\n\n\n\n\n\nBiodiversity\n\n\nGBIF\n\n\nDatacite\n\n\n\n\nHow to use the DataCite API to understand how GBIF counts citations\n\n\n\n\n\n\nJun 20, 2023\n\n\nBrett Johnson\n\n\n\n\n\n\n  \n\n\n\n\nHow do we mobilize salmon data?\n\n\n\n\n\n\n\nSalmon\n\n\necology\n\n\n\n\nCommunicate, assemble and coordinate extant data systems\n\n\n\n\n\n\nMay 30, 2023\n\n\nBrett Johnnson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A blog about data, science, and technology."
  },
  {
    "objectID": "posts/2023-06-20-GBIF/2023-06-20-How-does-gbif-count-citations.html",
    "href": "posts/2023-06-20-GBIF/2023-06-20-How-does-gbif-count-citations.html",
    "title": "How Does the Global Biodiversity Information System Count Citations?",
    "section": "",
    "text": "Introduction\nIf you publish your data to the Global Biodiversity Information Facility (GBIF), you may notice that your dataset is being cited by papers that seemingly have nothing to do with the context of your original project that produced the dataset.\nI had a colleague involved with the International Year of the Salmon (IYS) ask why IYS datasets such as one dataset about zooplankton was referenced in a paper that seemed to be about terrestrial species [@garcía-roselló2023].\nIf you look in the references of the Garcia-Rosello et al 2023 paper you will see two links to GBIF Occurrence Downloads. If you click on the DOI for GBIF.org, 2022b (https://doi.org/10.15468/dl.cqpa99), you will reach a landing page for an Occurrence Download from GBIF from a specific query. You can see the specific query filters that were used to generate the dataset.\nA total of 18,426 datasets that meet the query filter. The filter included: has CC BY 4.0 licence, Basis of record must be either Human observation, Observation or Preserved Specimen, Has coordinate is true, … Scientific name includes Animalia). As it turns out, this is a very broad filter basically asking for all the animal occurrences on GBIF datasets that have the right license and metadata and the dataset about zooplankton met all those criteria. If you wanted to see all the datasets included in the new dataset you can download a list of the involved datasets using the “Download as TSV” link from the GBIF Occurrence Dataset landing page for this big dataset and search for International Year of the Salmon to see which datasets are included.\n\nSo the dataset that resulted from this specific query includes occurrences meeting the query filter parameters from 18,426 other GBIF datasets receives a Digital Object Identifier. Each of those underlying datasets also has a digital object identifier and each of those DOIs is referenced in the new dataset’s DOI metadata as a ‘relatedIdentifier’.\n\nSo, each time the overarching dataset is cited, the citation count for each of the 18,426 datasets increases by one as well. Pretty cool, huh!?\n\nI can be surprising how the International Year of the Salmon data are re-used way outside the context of their collection, which is a major advantage of using the GBIF and publishing data using international standards.\nHere’s how you can access the metadata about the overarching DOI as well to see what kind of datasets it included.\n\n\nCode\nlibrary(rdatacite)\nlibrary(tidyverse)\n\n# download the DOI metadata for the new overarching dataset\ngbif_doi &lt;- dc_dois(ids = \"10.15468/dl.cqpa99\")\n\ndatasets_used &lt;- gbif_doi[[\"data\"]][[\"attributes\"]][[\"relatedIdentifiers\"]][[1]][\"relatedIdentifier\"]\n\n\nUsing this list of DOIs that the overarching dataset uses, we could retrieve all the names of those datasets, but let’s do something a bit more interesting.\nLet’s look at the International Year of the Salmon datasets and how they’ve been utilized by GBIF.\n\n\nCode\niys_dois &lt;- dc_dois(query = \"titles.title:International Year of the Salmon\", limit = 1000)\n\niys_citations &lt;- tibble(title = lapply(iys_dois$data$attributes$titles, \"[[\", \"title\"), citations = iys_dois[[\"data\"]][[\"attributes\"]][[\"citationCount\"]], doi = iys_dois[[\"data\"]][[\"attributes\"]][[\"doi\"]]) |&gt; \n  filter(citations &gt; 0)\n\niys_citations$title &lt;- substr(iys_citations$title, 4, 80)\n\ncites_iys_list &lt;- list()\n\nfor (i in iys_citations$doi) {\n  \n  x &lt;- dc_events(obj_id = paste0(\"https://doi.org/\", i))\n  cites_iys_list[[i]] &lt;- x\n}\n\n# Initialize lists to store objId and subjId\nobj_ids &lt;- list()\nsubj_ids &lt;- list()\n\n# Loop over the list to retrieve objId and subjId\nfor(i in 1:length(cites_iys_list)) {\n  data &lt;- cites_iys_list[[i]]$data$attributes\n  obj_ids[[i]] &lt;- data$objId\n  subj_ids[[i]] &lt;- data$subjId\n}\n\n# Flatten the lists\nobj_ids &lt;- unlist(obj_ids)\nobj_ids &lt;- substring(obj_ids, 17)\nsubj_ids &lt;- unlist(subj_ids)\nsubj_ids &lt;- substring(subj_ids, 17)\n\n# Get titles for objId and subjId\nobj_titles &lt;- rdatacite::dc_dois(ids = obj_ids, limit = 1000)\nsubj_titles &lt;- rdatacite::dc_dois(ids = subj_ids, limit = 1000)\n\nobj_dois &lt;- obj_titles[[\"data\"]][[\"attributes\"]][[\"doi\"]]\ntitle_list &lt;- obj_titles[[\"data\"]][[\"attributes\"]][[\"titles\"]]\ntitle_vector &lt;- unlist(map(title_list, function(x) x[['title']][1]))\nseq &lt;- as.character(1:length(obj_dois))\nobjects &lt;- tibble(position = seq, obj_dois, title_vector)\n\nsubj_dois &lt;- subj_titles[[\"data\"]][[\"attributes\"]][[\"doi\"]]\nsubjtitle_list &lt;- subj_titles[[\"data\"]][[\"attributes\"]][[\"titles\"]]\nsubjtitle_vector &lt;- unlist(map(subjtitle_list, function(x) x[['title']][1]))\nseq2 &lt;- as.character(1:length(subj_dois))\nsubjects &lt;- tibble(seq2, subj_dois, subjtitle_vector) |&gt; \n  filter(subjtitle_vector != \"Zooplankton Bongo Net Data from the 2019 and 2020 Gulf of Alaska International Year of the Salmon Expeditions\")\n\nsubj_related_ids &lt;- bind_rows(subj_titles[[\"data\"]][[\"attributes\"]][[\"relatedIdentifiers\"]], .id = \"position\") |&gt; \n  semi_join(objects, by = c('relatedIdentifier' = 'obj_dois')) |&gt; \n  left_join(subjects, by = c('position' = 'seq2')) |&gt; \n  filter(relationType != \"IsPreviousVersionOf\")\n  \n\n\nrelationships &lt;- full_join(objects, subj_related_ids, by = c('obj_dois' = 'relatedIdentifier'))\n\n\nlibrary(networkD3)\n\nobjects$type.label &lt;- \"IYS Dataset\"\nsubjects$type.label &lt;- \"Referencing Dataset\"\nids &lt;- c(objects$obj_dois,subjects$subj_dois)\nnames &lt;- c(objects$title_vector, subjects$subjtitle_vector)\ntype.label &lt;- c(objects$type.label, subjects$type.label)\n\nedges &lt;-tibble(from = relationships$obj_dois, to = relationships$subj_dois)\nlinks.d3 &lt;- data.frame(from=as.numeric(factor(edges$from))-1, \n                       to=as.numeric(factor(edges$to))-1 ) \nsize &lt;- links.d3 |&gt; \n   group_by(from) |&gt; \n  summarize(weight = n())\n\nnodes &lt;- tibble(ids, names, type.label) |&gt; \n  mutate(names = case_when(\n    names == \"Occurrence Download\" ~ paste0(names, \" \", ids),\n    TRUE ~ names\n    ),\n  )\n\nlength &lt;- nrow(nodes)\n\nmissing_length &lt;- as.integer(length) - nrow(size)\nmissing_size &lt;- rep.int(0, missing_length)\nsize &lt;- c(size$weight, missing_size)\n\nnodes$size &lt;- size\n\nnodes.d3 &lt;- cbind(idn=factor(nodes$names, levels=nodes$names), nodes) \n\nlibrary(networkD3)\nplot &lt;- forceNetwork(Links = links.d3, Nodes = nodes.d3, Source=\"from\", Target=\"to\",\n               NodeID = \"idn\", Group = \"type.label\",linkWidth = 1,\n               linkColour = \"#afafaf\", fontSize=12, zoom=T, legend=T, \n               Nodesize=\"size\", opacity = 0.8, charge=-300,\n               width = 600, height = 400)\n\nplot"
  }
]