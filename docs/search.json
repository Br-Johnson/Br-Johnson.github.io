[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A blog about data, science, and technology."
  },
  {
    "objectID": "posts/2023-06-20-GBIF/2023-06-20-How-does-gbif-count-citations.html",
    "href": "posts/2023-06-20-GBIF/2023-06-20-How-does-gbif-count-citations.html",
    "title": "How Does the Global Biodiversity Information System Count Citations from the International Year of the Salmon?",
    "section": "",
    "text": "Introduction\nIf you publish your data to the Global Biodiversity Information Facility (GBIF), you may notice that your dataset is being cited by papers that seemingly have nothing to do with the context of your original project that produced the dataset. For example, check out the citations of this zooplankton dataset.\nI had a colleague involved with the International Year of the Salmon (IYS) ask why this zooplankton dataset from the IYS expeditions was referenced in a paper that seemed to be about terrestrial species (garcía-roselló2023?). It’s not immediately obvious how the IYS zooplankton dataset was used in this paper, but if you dig in a little deeper it becomes clear, and frankly amazing, how GBIF integrates data and counts citations.\n\nHow was the IYS Zoop Data Used?\nIf you look in the references section of the Garcia-Rosello et al. (2023) paper you will see two links to GBIF Occurrence Downloads. If you click on the DOI for the second GBIF Occurrence Download dataset (GBIF.Org User 2022), you will reach a landing page for an Occurrence Download on GBIF from a specific query made by a user. You can even see the specific query filters that were used to generate the dataset.\n\n\n\nThe query filters generated by a GBIF user to return an integrated dataset\n\n\nA total of 18,426 datasets that meet the query filter. The filter included: has CC BY-NC 4.0 licence, Basis of record must be either Human observation, Observation or Preserved Specimen, Has coordinate is true, … Scientific name includes Animalia). As it turns out, this is a very broad filter basically asking for all the animal occurrences on GBIF datasets that have the right license and metadata and the IYS dataset about zooplankton met all those criteria. If you wanted to see all the datasets included in the new dataset you can download a list of the involved datasets using the “Download as TSV” link from the GBIF Occurrence Dataset landing page for this big dataset and search for International Year of the Salmon to see which datasets are included.\n\nSo the dataset that resulted from this specific query includes data from 18,426 other GBIF datasets which meet the query filter parameters. This new dataset receives a Digital Object Identifier and each of those underlying datasets also has a digital object identifier. GBIF is able to count citations because each of those 18,426 DOIs is referenced in the new dataset’s DOI metadata as a ‘relatedIdentifier’. So, each time the overarching dataset is cited, the citation count for each of the 18,426 datasets increases by one as well. Pretty cool, huh!?\n\nI can be surprising how the International Year of the Salmon data are re-used way outside the context of their collection, which is a major advantage of using the GBIF and publishing data using international standards. This also demonstrates the power of standardized data: new datasets can be integrated, downloaded, and identified with a DOI on the fly!\nIf you’re interested in finding all 18,427 Digital Object Identfiers and their titles or other metadata here’s how you could do that using the DataCite API and the rdatacite R package and searching for the DOI of the overarching dataset and extracting the relatedIdentifiers field.\n\n\nInternational Year of the Salmon Datasets\nFor a more interesting example, let’s look at how all the International Year of the Salmon datasets have been cited thus far. To do that we’ll take a similar approach but instead of searching for a specific DOI we’ll query for International Year of the Salmon in all the datacite DOIs, and extract each datasets relatedIdentifiers. We’ll then search for those relatedIdentifiers to retrieve their titles. Finally, I’ll join all that data together and present a network map of how each dataset connected by citations.\n\n\nCode\nlibrary(rdatacite)\nlibrary(tidyverse)\nlibrary(networkD3)\n\n# download the DOI metadata for the new overarching dataset\ngbif_doi &lt;- dc_dois(ids = \"10.15468/dl.cqpa99\")\n\ndatasets_used &lt;- gbif_doi[[\"data\"]][[\"attributes\"]][[\"relatedIdentifiers\"]][[1]][\"relatedIdentifier\"]\n\n# Get the data from 'International Year of the Salmon' titles\niys_dois &lt;- dc_dois(query = \"titles.title:International Year of the Salmon\", limit = 1000)\n\n# Create a tibble with the title, citation count, and DOI for each record, then filter by citation count greater than 0\niys_citations &lt;- tibble(\n  title = lapply(iys_dois$data$attributes$titles, \"[[\", \"title\"),\n  citations = iys_dois[[\"data\"]][[\"attributes\"]][[\"citationCount\"]],\n  doi = iys_dois[[\"data\"]][[\"attributes\"]][[\"doi\"]]\n) |&gt; filter(citations &gt; 0)\n\n# Reduce the title to the substring from the 4th to the 80th character\niys_citations$title &lt;- substr(iys_citations$title, 4, 80)\n\n# Initialize a list to store citation details of each DOI\ncites_iys_list &lt;- list()\n\n# Fetch citation details of each DOI and store it in the list\nfor (i in iys_citations$doi) {\n  x &lt;- dc_events(obj_id = paste0(\"https://doi.org/\", i))\n  cites_iys_list[[i]] &lt;- x\n}\n\n# Initialize lists to store objId and subjId\nobj_ids &lt;- list()\nsubj_ids &lt;- list()\n\n# Loop over the list to retrieve objId and subjId\nfor(i in 1:length(cites_iys_list)) {\n  data &lt;- cites_iys_list[[i]]$data$attributes\n  obj_ids[[i]] &lt;- data$objId\n  subj_ids[[i]] &lt;- data$subjId\n}\n\n# Flatten the lists and remove the prefix 'https://doi.org/'\nobj_ids &lt;- substring(unlist(obj_ids), 17)\nsubj_ids &lt;- substring(unlist(subj_ids), 17)\n\n# Get titles for objId and subjId\nobj_titles &lt;- rdatacite::dc_dois(ids = obj_ids, limit = 1000)\nsubj_titles &lt;- rdatacite::dc_dois(ids = subj_ids, limit = 1000)\n\n# Create a tibble of position, obj_doi, and its corresponding title\nobj_dois &lt;- obj_titles[[\"data\"]][[\"attributes\"]][[\"doi\"]]\ntitle_list &lt;- obj_titles[[\"data\"]][[\"attributes\"]][[\"titles\"]]\ntitle_vector &lt;- unlist(map(title_list, function(x) x[['title']][1]))\nseq &lt;- as.character(1:length(obj_dois))\nobjects &lt;- tibble(position = seq, obj_dois, title_vector)\n\n# Create a tibble of position, subj_doi, and its corresponding title\nsubj_dois &lt;- subj_titles[[\"data\"]][[\"attributes\"]][[\"doi\"]]\nsubjtitle_list &lt;- subj_titles[[\"data\"]][[\"attributes\"]][[\"titles\"]]\nsubjtitle_vector &lt;- unlist(map(subjtitle_list, function(x) x[['title']][1]))\nseq2 &lt;- as.character(1:length(subj_dois))\nsubjects &lt;- tibble(seq2, subj_dois, subjtitle_vector) |&gt; \n  filter(subjtitle_vector != \"Zooplankton Bongo Net Data from the 2019 and 2020 Gulf of Alaska International Year of the Salmon Expeditions\")\n\n# Get related identifiers and filter by obj_dois, join with subjects, and filter by relationType\nsubj_related_ids &lt;- bind_rows(subj_titles[[\"data\"]][[\"attributes\"]][[\"relatedIdentifiers\"]], .id = \"position\") |&gt; \n  semi_join(objects, by = c('relatedIdentifier' = 'obj_dois')) |&gt; \n  left_join(subjects, by = c('position' = 'seq2')) |&gt; \n  filter(relationType != \"IsPreviousVersionOf\")\n\n# Join objects and subj_related_ids by 'obj_dois' = 'relatedIdentifier'\nrelationships &lt;- full_join(objects, subj_related_ids, by = c('obj_dois' = 'relatedIdentifier'))\n\n# Prepare data for network plot\nlibrary(networkD3)\n\nobjects$type.label &lt;- \"IYS Dataset\"\nsubjects$type.label &lt;- \"Referencing Dataset\"\nids &lt;- c(objects$obj_dois,subjects$subj_dois)\nnames &lt;- c(objects$title_vector, subjects$subjtitle_vector)\ntype.label &lt;- c(objects$type.label, subjects$type.label)\n\n# Create edges for network plot\nedges &lt;-tibble(from = relationships$obj_dois, to = relationships$subj_dois)\nlinks.d3 &lt;- data.frame(from=as.numeric(factor(edges$from))-1, \n                       to=as.numeric(factor(edges$to))-1 ) \nsize &lt;- links.d3 |&gt; \n   group_by(from) |&gt; \n  summarize(weight = n())\n\nnodes &lt;- tibble(ids, names, type.label) |&gt; \n  mutate(names = case_when(\n    names == \"Occurrence Download\" ~ paste0(names, \" \", ids),\n    TRUE ~ names\n    ),\n  )\n\nlength &lt;- nrow(nodes)\n\nmissing_length &lt;- as.integer(length) - nrow(size)\nmissing_size &lt;- rep.int(0, missing_length)\nsize &lt;- c(size$weight, missing_size)\n\nnodes$size &lt;- size\n\nnodes.d3 &lt;- cbind(idn=factor(nodes$names, levels=nodes$names), nodes) \n\n# Create and render the network plot\nlibrary(networkD3)\nplot &lt;- forceNetwork(Links = links.d3, Nodes = nodes.d3, Source=\"from\", Target=\"to\",\n               NodeID = \"idn\", Group = \"type.label\",linkWidth = 1,\n               linkColour = \"#afafaf\", fontSize=12, zoom=T, legend=T, \n               Nodesize=\"size\", opacity = 0.8, charge=-300,\n               width = 600, height = 400)\n\nplot\n\n\n\n\n\n\nIn summary, the DataCite REST API offers a lot of great details about datasets published with a DOI. Using this service you can understand how your data are being used in a programmatic way that would be easy to create a dashboard with.\nOne limitation to Datacite’s REST API, however, is that it only indexes DOIs minted by Datacite and not by other services such as CrossRef which mints DOIs mainly for journal articles.\nThankfully, DataCite also offers a GraphQL API which indexes not only DataCite and Crossref, but also ORCID, and ROR! So, stay tuned for a future blog post demonstrating the use of this amazing service.\n\n\n\n\n\n\n\n\nReferences\n\nGBIF.Org User. 2022. “Occurrence Download.” The Global Biodiversity Information Facility. https://doi.org/10.15468/DL.CQPA99."
  },
  {
    "objectID": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html",
    "href": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html",
    "title": "Google’s Business Intelligence Certificate Overview",
    "section": "",
    "text": "Business intelligence aims to leverage data to generate actionable insights that improve business outcomes by way of increasing organizational data maturity. BI us multifaceted combining data analytics, software development, project management and business analysis to inform strategic, operational, and predictive business decisions.\nSkills I acquired from [Google’s Business Intelligence Certificate](https://www.coursera.org/professional-certificates/google-business-intelligence#courses) include:\n\nData engineering: Acquired deeper insights into numerous database types, design schema, and performance optimization. Used Google Cloud Platform and Big Query.\nAnalytics engineering: Developed robust ETL data pipelines for real-time analytics using Google DataFlow, SQL, and Python.\nIntelligence reporting: Mastered automated report and dashboard creation, utilizing Tableau, Python’s streamlit package, as well as R’s shiny.\nIntelligence life cycle management: Learned stakeholder engagement, information needs assessment, and problem-solving through design thinking principles.\n\nThe certificate has increased my technical capacity as well as my business acumen and stakeholder management capabilities. It has empowered me with the knowledge and skills to advance my organization’s goals with a higher level of professionalism and sophistication, further positioning us to elevate data-driven initiatives."
  },
  {
    "objectID": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#bi-stakeholders",
    "href": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#bi-stakeholders",
    "title": "Google’s Business Intelligence Certificate Overview",
    "section": "BI Stakeholders",
    "text": "BI Stakeholders\nIt can be helpful to think about the various categories of stakeholders in a BI project to better understand and articulate their needs.\nProject Sponsors are ultimately responsible and accountable for the project and often establish criteria for success.\nApplication Developers generate data and consume data and are important to work with to ensure useful data streams are identified.\nSystems analysts design and implement information systems, often back end platforms, and have a big picture in mind for how information needs to move throughout the business.\nBusiness stakeholders including executives, customer service facing employees, data science teams made up of analysts and engineers will have different requirements for information\nWhile it is important to meet with key stakeholders individually and even observe teams in action to understand the what, how and why of what they do, holding stakeholder alignment workshops is a great strategy to ensure stakeholder requirements and project requirements are aligned within the business."
  },
  {
    "objectID": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#bi-outputs-and-processes",
    "href": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#bi-outputs-and-processes",
    "title": "Google’s Business Intelligence Certificate Overview",
    "section": "BI Outputs and Processes",
    "text": "BI Outputs and Processes\nMuch of BI is about bringing various sources of data together in a way that allows them to be integrated and analyzed. The end goal is often a suite of metrics to understand what is happening in the market, in the business, or in your organization in real time. Distilling down a deluge of information into informative visualizations is the bread and butter of BI. To do that BI professionals often use ‘Extract, Transform, Load’ data pipelines that feed data into a suite of visualizations such as a dashboard, or reproducible reports.\nThe type of metrics used will depend on if its one of the three types of dashboards: 1) Strategic; 2) Operational; 3) Analytical.\nStrategic dashboards usually include Key Performance Indicators (KPIs) and a “North Star” metric. A North Star metric is the most important measure of success, and is important for organizations to define because it can align the business and the employees. Strategic dashboards track the trajectory and progress of the business over the medium to long term.\nOperational dashboards, on the other hand, offer real time insights for rapid intelligence gathering and decision making. Monitoring various aspects of a business or market which can change rapidly over the course of hours, days, or weeks. Examples might include number of sales, website up time, website traffic or customer complaints.\nAnalytical dashboards offer a deeper dive in various aspects of business data, that might allow a user to tweak some assumptions of underlying models to better predict future scenarios.\n\nBI Planning Documents\nWhatever the dashboard type, going through a standard process for gathering stakeholder requirements is essential. Three documents can ensure a BI project delivers value:\n1) Stakeholder Requirements Document is usually one-page and defines a) the business problem; b) key stakeholders names and job titles; c) primary requirements to consider the project a success.\n2) Project Requirements Document is longer and outlines why the project needs doing and why the company should invest in it.\n\nIt defines key dependencies: what data do we currently have versus what new data collections might be needed\nIdentifies major elements of the project, the team and roles within each team\nDefine a minimum viable product that should be built before embarking on more challenging aspects of the project such as new data collection procedures\nDefines the expected deliverables and success using SMART criteria\n\nIdentify key metrics with necessary technologies and processes that are already in place\nIdentify the frequency of delivery of supporting data for key metrics to identify misalignment\n\nPrioritizes features based on stakeholder requirements but also based on features with fewest dependencies\nDocuments ‘User Journeys’, ie. the current user experience and the future ideal experience\nIncludes privacy, access, legal and compliance concerns and requirements. Who needs access?\nExplicitly states any assumptions\nAccessibility: defines key considerations for accessibility; optimized for mobile or desktop? Colour blind? Large text?\nUser-support framework: How will users be empowered to contribute data sustainably and learn to use the dashboard?\nRoll-out plan: Describe scope, priorities, timeline. At what point during the roll out will measurements be made to ensure the features are performing as expected?\nKey references: Documents, policies, how to guides, links to project tracking interface\n\n3) Strategy document offers a collaborative space to align stakeholders about project deliverables.\n\nDetails of dashboard functionality, metrics, charts\nFleshes out limitations or assumptions about data\nProvides chart and dashboard mock ups\nRequest review and sign-off from key stakeholders to complete the planning phase.\n\n\n\nBI is All About Communication\nWhile it might sound cliche or rote to say communication is key, but many of us fail to understand this and suffer from misunderstandings, wasting time, and not generating the right value as a result. Intelligence is about information, how do we collect information? How do we utilize information? Communication. Communication from the technical foundation of internet protocols, APIs, data pipelines, to the human dimensions required to gather stakeholder requirements, give and receive feedback, and inform others what your newly acquired intelligence actually means. So, it’s not a matter of prioritizing communication for effective BI it about realizing that BI is communication, not just fancy tools and pretty pictures.\nTo effectively communicate in BI there are some basic considerations: communicate early and communicate often. Early communication by way of scoping stakeholder requirements, and often by way of formalized check in points. Timing and frequency is critical, but so are the methods. Asking questions is one of the best methods of communication so let’s consider how to do that most effectively:\n\nAsk effective questions\n\nAsk open-ended questions\nDon’t ask leading questions\nBe curious rather than making a point with a passive aggressive question\nDig deeper into confusing or unclear situations by asking clarifying questions\nAsk specific questions not vague ones\nAsk SMART questions: Specific, measurable, actionable, relevant, time-bound\n\n\nWhen presenting information in the form of presentation or meeting, use the who what when where why and how formula.\nKeeping track of project milestones and tracking progress towards to milestones in a project change log is a great way to stay on top of key changes that will need to be communicated. Make a plan to communicate changes from the project change log often."
  },
  {
    "objectID": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#bi-databases",
    "href": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#bi-databases",
    "title": "Google’s Business Intelligence Certificate Overview",
    "section": "BI Databases",
    "text": "BI Databases\nThere’s a lot of talk of different database types within the field of business intelligence and analytics. I’ll try to clarify some of the more common types that I found useful to understand coming from a background in biology and scientific data where the relational database is the main workhorse. What’s the difference between data lakes, data marts, data warehouses, graph databases, and more recently vector databases?\nFirst, let’s start with the difference between a database and a data warehouse.\nDatabase: Simply put a database is a collection of data stored in a computer system. They can be optimized for many purposes, but a traditional database optimized to minimize duplication, reduce storage requirements, and maintain data integrity through a process of normalization. Normalization in essence is the process of breaking data into multiple tables so that no column has a repeated row of data. Tables are then joined by identifier columns with unique values that match other tables referred to as keys. In this way each table is related some how to others, hence the term relational database. While very efficient and logical, the design of a databases structure (schema) is defined early in the design process and remains rather rigid and difficult to change later which is a significant drawback to the typical relational database. Typical examples of relational databases include PostgreSQL, MySQL, SQLite, MongoDB, and Microsoft Access.\nData warehouses: Are a type of database that consolidate data from multiple source systems aimed at ensuring consistency, accuracy, and efficient queries and and analysis. They often rely on denormalized designs such as star or snowflake schema, which facilitate faster query performances because the data are organized around important business facts or metrics and fewer tables may need to be joined. Examples of data warehouses include Google Big Query, Amazon Redshift, and Snowflake.\nOne of the key differences between a data warehouse and a relational database is that data warehouses are optimized for column-based operations which allows data to be spread across multiple servers and can increase query performance. Databases on the other hand are optimized for row-based operations, meaning that an entire row of a database has to be retrieved and scaling has to be done horizontally by increasing the capacity in a single server.\nData Lakes allow you store both structured and unstructured (videos, images etc) data at any scale. Data can be stored raw, ie in it’s original format, without having to fit into a schema thereby offering great flexibility. They are flexible and low cost but can make it difficult to access useful information can become chaotic without adequate governance and queries can be very slow due to a lack of structure.\nData Marts are subject specific databases that are optimized for specific business users. Unlike data lakes, they are structured and contain only relevant data for specific uses.\nOLTP (online transaction processing) databases are optimized processing and managing transnational data with an emphasis on fast query processing and maintaining integrity in high contention (multi-access) and high concurrency environments. Often organized in a normalized relational model. MySQL, Oracle and Microsoft SQL Server are all common examples OLTP databases.\nOLAP (online analytical processing) databases are designed for complex queries and calculations usually for the purpose of real time analytics. Often organized as a de-normalized star or snowflake schema or in columnar databases for faster query performance. Generally concurrency and contention is not an issue because a single team or user are utilizing the databases at a time. SAP BW, IBM Cognos Analytics and Microsoft Analysis Services are popular commercial OLAP databases.\nGraph databases are a huge topic and likely warrant their own blog post. There’s a lot of interest and perhaps hype surrounding graph databases. Briefly graph databases are designed to store data in the form graphs, which is the connection of nodes via edges. Graph databases are schema-less meaning the structure of the graph can be extended naturally as new relationships or nodes are needed. This offers a lot of flexibility compared to traditional relational databases. However, there’s a notion of universal applicability that graph databases can solve all relational problems, which is not true. They are often marketed as a panacea for big data complexities but scalability can still be an issue. In reality graph databases are extremely effective for niche applications and problems that are intrinsically based on relationships. However, implementing a graph database and migrating data into it can be challenging and complex.\nVector Databases have recently emerged as a critical data storage environment related to artificial intelligence. As the name implies, they store vectors often used to represent geometric objects, such as points, lines, and polygons but vectors can often represent complex entities that have been transformed into high dimensional spaces via machine learning models, such as deep neural networks. They can be useful for Content-based recommendation systems, and finding similar items based on their high dimensional feature vectors. They are also useful for natural language, processing computer vision, and anomaly detection. Lastly, vector databases are critical in storing embeddings required for large language models. They are highly scalable and enable real time applications like chat, bots, or recommendation systems to utilize large language models effectively.\nFeature-oriented databases represent a nuanced approach to database design and data storage that specifically caters to the management of important features or attributes of data. These features may be determined by subject matter experts, or extracted from text or image data using various feature extraction processes such as natural language processing or computer vision. These features are there in turn used to train machine learning models. Many AI systems depend on this type of database because the features are in a model-ready format."
  },
  {
    "objectID": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#design-thinking-and-structured-thinking",
    "href": "posts/2023-09-05-Business Intelligence in Science/BI-in-science.html#design-thinking-and-structured-thinking",
    "title": "Google’s Business Intelligence Certificate Overview",
    "section": "Design Thinking and Structured Thinking",
    "text": "Design Thinking and Structured Thinking\nYou are probably familiar with structured thinking already whether you realize it or not. It’s essentially the scientific process that includes stages: problem definition, hypothesis generation, data collection, analysis and solutions. When you encounter complex problems while working on BI projects “Design Thinking” can be a very useful method to innovate solutions that are focused on user needs. Design thinking differs from structured thinking in that it takes a human-centred, iterative, explorative, and flexible approach whereas structured thinking is less collaborative, more linear, and mostly uses deductive reasoning to find the most efficient solutions.\nDesign thinking shines for:\n1) tackling uncertain and ambiguous problems when the problem-space is not well defined or when dealing with ‘wicked problems’; 2) user-centric projects; 3) Innovation; 4) Interdisciplinary challenges; and 5) Rapid prototyping and iteration.\nMany problems in environmental science are ‘wicked problems’ that don’t have a clear definition, include many stakeholders with different perspectives, complex inter-dependencies, don’t have a clear way to establish a definitive solution and have incomplete, contradictory, or changing requirements. When a purely scientific-engineering approach to solving a problem using structured thinking does not or will not work, design thinking may offer useful solutions.\n\nKey Stages of Design Thinking:\n\nEmpathize: This initial stage involves understanding the emotional experiences and needs of the end-user. Research methods may include interviews, observations, and immersion in the user environment.\nDefine: Based on the empathy stage, the problem is explicitly defined from the user’s perspective. The goal is to articulate what the user needs, not what the business needs.\nIdeate: This stage involves generating a wide variety of potential solutions through brainstorming and other ideation techniques. The focus is on volume and diversity of ideas.\nPrototype: Ideas are turned into low-fidelity prototypes, which could range from sketches to physical models. The aim is to visualize potential solutions and prepare them for testing.\nTest: The prototype is presented to end-users, and their interactions and feedback are closely observed. This data is then used to refine the solution.\n\n\n\nWhen to Apply Structured Thinking\n\nWell-Defined Problems: When you have a specific, concrete problem to solve and the criteria for success are clear.\nData-Driven Decisions: When solutions can be evaluated empirically or quantitatively.\nLogical Complexity: When the problem can be decomposed into smaller problems that require systematic analysis.\nOperational Efficiency: When the main goal is to improve the efficiency of an existing process or system.\nRisk Mitigation: When it’s critical to make decisions that minimize risk and error."
  },
  {
    "objectID": "posts/2023-05-30-Mobilizing Salmon Data/index.html",
    "href": "posts/2023-05-30-Mobilizing Salmon Data/index.html",
    "title": "How do we mobilize salmon data?",
    "section": "",
    "text": "Last updated: 2023-06-29"
  },
  {
    "objectID": "posts/2023-05-30-Mobilizing Salmon Data/index.html#whos-already-publishing-salmon-data",
    "href": "posts/2023-05-30-Mobilizing Salmon Data/index.html#whos-already-publishing-salmon-data",
    "title": "How do we mobilize salmon data?",
    "section": "Who’s already publishing salmon data?",
    "text": "Who’s already publishing salmon data?\nLet’s look through the datasets registered with DataCite that have the word salmon somewhere in their DOI’s metadata. To do this I will use DataCite’s REST API and the R package rdatacite which provides convenient functions for making API calls.\n\n\nCode\nlibrary(rdatacite)\nlibrary(tidyverse)\nlibrary(plotly)\n# Initialize lists to store dataframes and metadata\nall_datasets &lt;- list()\nall_meta &lt;- list()\n\n# Fetch initial page\ntt_salmon_datasets &lt;- dc_dois(query = \"salmon\")\n\ntt_providers &lt;- tt_salmon_datasets[[\"meta\"]][[\"providers\"]]\n# tt_affiliations &lt;- tt_salmon_datasets[[\"meta\"]][[\"affiliations\"]][[\"title\"]]\n\nplotly::ggplotly(ggplot2::ggplot(tt_providers, aes(x = reorder(title, count), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(x = \"Publisher\", y = \"Count\") +\n  theme_minimal() +\n  ggtitle(\"Top Ten Publishers of Salmon Datasets\")) \n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\ntt_affiliations &lt;- tt_salmon_datasets[[\"meta\"]][[\"affiliations\"]]\n\nggplot(tt_affiliations, aes(x = reorder(title, count), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(x = \"Institutions\", y = \"Count\") +\n  theme_minimal() +\n  ggtitle(\"Top Ten Institutions Providing Salmon Datasets\")\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\nyears_published &lt;- tt_salmon_datasets[[\"meta\"]][[\"published\"]]\n\nggplot(years_published, ggplot2::aes(x = id, y = count))+\n                   geom_bar(stat = \"identity\", fill = \"steelblue\")+\n                   labs(x = \"Year\", y = \"Count\") +\n                   coord_flip() +\n                   theme_minimal() +\n                   ggtitle(\"Growth of salmon datasets published with a DOI\")"
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "",
    "text": "Controlled vocabularies and ontologies are cornerstone elements in the domain of information organization. By establishing a standardized lexicon and defining the relationships between concepts, these frameworks empower sectors of society and research domains to achieve greater data consistency, interoperability, and semantic precision.\n\n\nThe journey of controlled vocabularies and ontologies began in the early days of library science, evolving from simple thesauri and classification systems to sophisticated semantic frameworks. The inception of the semantic web in the late 20th century marked a significant leap, propelling the development of ontologies to enable the web of data to be machine-readable. Standards such as RDF (Resource Description Framework), OWL (Web Ontology Language), and SKOS (Simple Knowledge Organization System) emerged, each contributing to the layered architecture of the semantic web, enabling machines to understand and process the semantics of data.\nBy transitioning from a very basic controlled vocabulary in a spreadsheet to a structured ontology using Python and RDFlib, this tutorial underscores a practical application of these concepts. Through this tutorial, you will learn how to get started building an ontology without any prior experience, equipping you with the knowledge to implement these powerful tools in your domain, thereby enhancing the richness and utility of your data.\nWe will use the following tools and technologies in this tutorial:\n\nPython: A versatile programming language widely used for data processing and manipulation.\npandas: A powerful data manipulation library in Python that provides data structures and functions for working with structured data.\nRDFlib: A Python library for working with RDF (Resource Description Framework) data, which is commonly used for representing ontologies and semantic data.\ngooglesheets: A Google Sheets document that contains a simple controlled vocabulary.\nprotégé: An ontology editor and knowledge management tool that we will use to visualize and explore the created ontology.\n\nand these standards:\n\nRDF: The Resource Description Framework is a foundational standard for encoding, exchanging, and reusing structured metadata. Its graph-based structure aligns well with the interconnected nature of semantic data, making it a core pillar of the semantic web.\nSKOS: Simple Knowledge Organization System provides a simpler, lightweight standard compared to OWL, ideal for representing controlled vocabularies. Its ease of use and focus on basic semantic relationships make it suitable for beginners and for applications where complex ontology features are unnecessary.\nOWL: The Web Ontology Language extends RDF, offering more advanced features for describing properties and classes, such as relations between classes, cardinality, equality, and more. Its selection for this tutorial is due to its expressiveness and widespread adoption in ontology development.\nturtle: A human-readable serialization format for RDF data that uses a compact syntax to represent triples."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#historical-insight-into-controlled-vocabularies-and-ontologies",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#historical-insight-into-controlled-vocabularies-and-ontologies",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "",
    "text": "The journey of controlled vocabularies and ontologies began in the early days of library science, evolving from simple thesauri and classification systems to sophisticated semantic frameworks. The inception of the semantic web in the late 20th century marked a significant leap, propelling the development of ontologies to enable the web of data to be machine-readable. Standards such as RDF (Resource Description Framework), OWL (Web Ontology Language), and SKOS (Simple Knowledge Organization System) emerged, each contributing to the layered architecture of the semantic web, enabling machines to understand and process the semantics of data.\nBy transitioning from a very basic controlled vocabulary in a spreadsheet to a structured ontology using Python and RDFlib, this tutorial underscores a practical application of these concepts. Through this tutorial, you will learn how to get started building an ontology without any prior experience, equipping you with the knowledge to implement these powerful tools in your domain, thereby enhancing the richness and utility of your data.\nWe will use the following tools and technologies in this tutorial:\n\nPython: A versatile programming language widely used for data processing and manipulation.\npandas: A powerful data manipulation library in Python that provides data structures and functions for working with structured data.\nRDFlib: A Python library for working with RDF (Resource Description Framework) data, which is commonly used for representing ontologies and semantic data.\ngooglesheets: A Google Sheets document that contains a simple controlled vocabulary.\nprotégé: An ontology editor and knowledge management tool that we will use to visualize and explore the created ontology.\n\nand these standards:\n\nRDF: The Resource Description Framework is a foundational standard for encoding, exchanging, and reusing structured metadata. Its graph-based structure aligns well with the interconnected nature of semantic data, making it a core pillar of the semantic web.\nSKOS: Simple Knowledge Organization System provides a simpler, lightweight standard compared to OWL, ideal for representing controlled vocabularies. Its ease of use and focus on basic semantic relationships make it suitable for beginners and for applications where complex ontology features are unnecessary.\nOWL: The Web Ontology Language extends RDF, offering more advanced features for describing properties and classes, such as relations between classes, cardinality, equality, and more. Its selection for this tutorial is due to its expressiveness and widespread adoption in ontology development.\nturtle: A human-readable serialization format for RDF data that uses a compact syntax to represent triples."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#benefits-of-using-controlled-vocabularies-and-ontologies",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#benefits-of-using-controlled-vocabularies-and-ontologies",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Benefits of Using Controlled Vocabularies and Ontologies",
    "text": "Benefits of Using Controlled Vocabularies and Ontologies\n\nImproved Data Consistency: By using standardized terms and definitions, you can ensure consistency and coherence across different data sources and applications.\nEnhanced Search and Retrieval: Controlled vocabularies enable more accurate and efficient search and retrieval of information by providing a common language for indexing and querying.\nInteroperability: Ontologies promote interoperability by establishing a shared understanding of concepts and relationships, facilitating data integration and exchange between systems.\nSemantic Enrichment: By capturing the semantics of data through ontologies, you can add context and meaning to information, enabling more sophisticated data analysis and reasoning."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#importing-necessary-libraries",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#importing-necessary-libraries",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Importing Necessary Libraries",
    "text": "Importing Necessary Libraries\nimport pandas as pd\nfrom rdflib import Graph, Literal, Namespace, URIRef\nfrom rdflib.namespace import RDF, RDFS, SKOS\nrdflib is a Python library for working with RDF, a standard model for data interchange on the Web.\nRDF is based on the idea of making statements about resources (in particular web resources) in the form of subject-predicate-object expressions. These expressions are known as triples in RDF terminology. The subject denotes the resource, and the predicate denotes traits or aspects of the resource, and expresses a relationship between the subject and the object. This simple model is used to represent knowledge in a machine-readable way and is the basis for the Semantic Web, linked data, and knowledge graphs.\nEach import from rdflib illustrates a different concept in the RDF data model:\n\nGraph: Represents an RDF graph, the fundamental structure used to store triples in RDF. Each triple consists of a subject, predicate, and object, mirroring the basic entity-attribute-value model in knowledge representation.\nLiteral: Represents a literal value in RDF, such as a string, number, or date. Literals are used to represent data values rather than resources.\nNamespace: In RDF, namespaces are used to provide prefixes for URIs, avoiding collisions between terms from different vocabularies. It’s crucial for ensuring consistency and disambiguation in knowledge representation.\nURIRef: Represents a URI reference in RDF, used to identify resources and concepts. URIs are the backbone of the Semantic Web, providing unique identifiers for entities and enabling the creation of linked data.\nRDF: The namespace for RDF itself, providing access to core RDF terms like rdf:type. RDF is the backbone of semantic web technologies, allowing for the modeling of knowledge in a machine-readable format.\nRDFS: The namespace for RDF Schema, a vocabulary extension of RDF that provides additional terms for defining classes, properties, and relationships. RDFS is used to create simple ontologies and taxonomies.\nSKOS: The Simple Knowledge Organization System is a standard for representing controlled vocabularies, taxonomies, and thesauri in RDF. SKOS provides a lightweight, easy-to-use framework for organizing and managing knowledge resources. I include it here because I use the term skos:definition in the ontology to define the terms.\n\nIn short, rdflib includes everything you need to parse, create, and manipulate RDF data, serialize it to different formats, and even perform reasoning tasks on RDF graphs."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#reading-data-from-google-sheets",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#reading-data-from-google-sheets",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Reading Data from Google Sheets",
    "text": "Reading Data from Google Sheets\nurl = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vS7cXXTu71hmQVbt5zvtLJAEqyI4t1zhEzGBGXTH38o-QlBEflNXY1PugOC2iJ4J0BN1Ocp6hODzrW1/pub?gid=0&single=true&output=csv\"\ndf = pd.read_csv(url)\ndf.to_csv(\"./data/sdm_vocab.csv\", index=False)\nHere, the script reads data from a Google Sheet via a URL. You can publish any googlesheet file to be publicly accessible like this by selecting File &gt; Share &gt; Publish to Web in any googlesheet.\n\nThe data is read directly into a pandas DataFrame using the pd.read_csv() function. The DataFrame is then saved to a local CSV file to version control in GitHub."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#defining-namespaces",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#defining-namespaces",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Defining Namespaces",
    "text": "Defining Namespaces\nnamespaces = {\n    \"rdf\": Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"),\n    \"owl\": Namespace(\"http://www.w3.org/2002/07/owl#\"),\n    \"xml\": Namespace(\"http://www.w3.org/XML/1998/namespace\"),\n    \"xsd\": Namespace(\"http://www.w3.org/2001/XMLSchema#\"),\n    \"rdfs\": Namespace(\"http://www.w3.org/2000/01/rdf-schema#\"),\n    \"skos\": Namespace(\"http://www.w3.org/2004/02/skos/core#\"),\n}\nThis section defines the namespaces to be used in the RDF graph. Namespaces in RDF are a mechanism to avoid element name conflicts.\nNamespaces are a key aspect of RDF as they allow for the disambiguation of resources. They are typically URIs and are used as a prefix to the local name of a resource. This allows resources to be uniquely identified even if their local names clash."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#creating-the-rdf-graph",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#creating-the-rdf-graph",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Creating the RDF Graph",
    "text": "Creating the RDF Graph\ng = Graph()\nfor prefix, namespace in namespaces.items():\n    g.bind(prefix, namespace)\nAn empty RDF graph g is created using the Graph() function from rdflib. The namespaces defined earlier are then bound to the graph.\nAn RDF graph is a set of RDF triples. Each RDF triple consists of a subject, a predicate, and an object. The subject denotes the resource, and the predicate denotes traits or aspects of the resource and expresses a relationship between the subject and the object."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#adding-data-to-the-graph",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#adding-data-to-the-graph",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Adding Data to the Graph",
    "text": "Adding Data to the Graph\nfor _, row in df.iterrows():\n    class_uri = URIRef(f\"http://example.org/{row['Name'].replace(' ', '_')}\")\nFinally, the script iterates over the rows of the DataFrame. For each row, it creates a URI for a class, replacing spaces in the name with underscores. This URI is then added to the graph.\nA URI, or Uniform Resource Identifier, is a string of characters that unambiguously identifies a particular resource. In the context of RDF, URIs are used to both identify the subject of a statement and to express the relationship between a subject and an object."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#adding-class-to-the-graph",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#adding-class-to-the-graph",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Adding Class to the Graph",
    "text": "Adding Class to the Graph\ng.add((class_uri, RDF.type, namespaces[\"owl\"].Class))\nHere, the script adds the class to the graph. The RDF.type property is used to state that the subject is an instance of a class.\nOWL, or Web Ontology Language, is a Semantic Web language designed to represent rich and complex knowledge about things, groups of things, and relations between things. In this case, the script is stating that the class URI is an instance of an OWL class.\nOWL extends RDF and is widely used in ontology development and knowledge representation and improves the expressiveness and reasoning capabilities of RDF."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#adding-class-label",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#adding-class-label",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Adding Class Label",
    "text": "Adding Class Label\ng.add((class_uri, RDFS.label, Literal(row[\"Name\"])))\nThe script then adds the name of the class as a label. The RDFS.label property is used to provide a human-readable version of a resource’s name.\nRDFS, or RDF Schema, provides a basic type system for RDF models. It is used to define classes, properties, and their interrelationships."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#adding-class-definition",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#adding-class-definition",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Adding Class Definition",
    "text": "Adding Class Definition\ng.add((class_uri, SKOS.definition, Literal(row[\"Definition\"])))\nThe script then adds the definition of the class. The SKOS.definition property is used to provide a definition of the resource.\nSKOS, or Simple Knowledge Organization System, is a W3C recommendation designed for representation of thesauri, classification schemes, taxonomies, subject-heading systems, or any other type of structured controlled vocabulary.\nYou might be confused about how this ontology uses RDF, RDFS, SKOS, and OWL. The key point is that these are all parts of the Semantic Web stack, and they can be used together to create rich and expressive ontologies. RDF provides the basic data model, RDFS provides a basic vocabulary for describing classes and properties, SKOS provides a vocabulary for representing controlled vocabularies, and OWL provides a more expressive language for defining classes, properties, and relationships. By combining these different parts of the Semantic Web stack, you can create ontologies that are both powerful and flexible."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#serializing-the-graph",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#serializing-the-graph",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Serializing the Graph",
    "text": "Serializing the Graph\nturtle_data = g.serialize(format=\"turtle\")\nThe script then serializes the graph in Turtle format. Turtle is a syntax and file format for expressing data in the RDF data model in a way that is easier to read and write than RDF/XML."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#writing-the-graph-to-a-file",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#writing-the-graph-to-a-file",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Writing the Graph to a File",
    "text": "Writing the Graph to a File\nwith open(\"./outputs/sdm_vocab.ttl\", \"w\") as f:\n    f.write(turtle_data)\nFinally, the script writes the serialized data to a .ttl file. This allows the RDF data to be stored and shared."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#expanding-the-ontology-with-additional-concepts-and-relationships",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#expanding-the-ontology-with-additional-concepts-and-relationships",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Expanding the Ontology with Additional Concepts and Relationships",
    "text": "Expanding the Ontology with Additional Concepts and Relationships\nOnce you have imported the ontology into Web Protégé, you can further expand and refine it by adding additional concepts, relationships, and axioms. You can define new classes, properties, and restrictions to capture more complex domain knowledge and semantics. By iteratively refining the ontology based on feedback and domain expertise, you can create a comprehensive and semantically rich representation of the domain.\n\nKeep in mind, you can always update the controlled vocabulary in the Google Sheets document, rerun the Python script to generate a new version of the ontology, and import it back into Web Protégé for further refinement. This iterative process allows you to continuously improve and evolve the ontology based on new contributions without having all your collaborators sign up for a web protégé account. However, if you’ve added more complext relationships or axioms, you will need to reflect that in the google sheet and adjust your python script, or if you want to collaborate in real-time, you can invite your collaborators to join the ontology project in Web Protégé."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#emergent-capabilities-from-ontology-development",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#emergent-capabilities-from-ontology-development",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Emergent Capabilities from Ontology Development",
    "text": "Emergent Capabilities from Ontology Development\nOnce the “Salmon Hatchery Production” ontology is established and integrated with other relevant ontologies, it unlocks a range of advanced capabilities for researchers, policymakers, and industry stakeholders:\n\nData Integration and Analysis: By integrating diverse datasets using the ontology for meta analyses, scientists would save a tremendous amount of time avoiding data wrangling tasks and have clearly defined data that’s easy to understand. They could then conduct comprehensive analyses that reveal hidden patterns, correlations, and causal relationships within the salmon production ecosystem at scales not easily accessed before.\nApplications of Artificial Intelligence: Machine learning algorithms could be trained on the ontology-enhanced datasets to predict optimal hatchery conditions, forecast salmon population dynamics, and identify potential risks or bottlenecks in the production process.\nEnhancing Large Language Models with Knowledge Graph: Advanced language models like Chat GPT could leverage the ontology to generate expert level insights, recommendations, and reports on salmon hatchery production. Knowledge graphs provide structured knowledge compared to vector embeddings, typically used in Large Language Models, which provide unstructured knowledge. Think of an LLM without a salmon hatchery knowledge graph as a high school student with only Biology 11, and with the knowledge graph as a PhD graduate with a deep understanding of salmon hatchery production.\nApplication Development: Developers could use the ontology to build applications that provide real-time insights into hatchery operations, predict outcomes based on environmental conditions, and optimize production strategies for sustainable salmon populations.\nSemantic Search and Discovery: Data stewards or providers could annotate datasets stored in data catalogues with ontology terms, to allow users to perform semantic searches that go beyond keyword matching, uncovering relevant information based on the underlying semantics and relationships encoded in the ontology and detected in potentially useful datasets using semantic search algorithms.\nPolicy Compliance and Reporting: Governments and regulatory bodies could use the ontology to standardize reporting requirements, ensuring that data submitted by different entities are consistent, comparable, and compliant with regulations.\nInternational Collaboration: A globally shared ontology could facilitate international collaboration on salmon research, conservation, and management, enabling knowledge exchange and harmonizing practices across borders.\nOpen Science: An open-access ontology could promote transparency, reproducibility, and collaboration in salmon research, fostering a culture of open science that accelerates innovation and discovery.\n\nBy developing and integrating advanced ontologies like the “Salmon Hatchery Production” ontology with broader domain ontologies, researchers and stakeholders can unlock new insights, capabilities, and opportunities for addressing complex challenges in salmon conservation, aquaculture, and ecosystem management."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#common-challenges",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#common-challenges",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Common Challenges",
    "text": "Common Challenges\n\nComplexity Management: As ontologies grow, they can become complex and difficult to manage. Regular refactoring and modularization can help manage this complexity.\nEvolving Requirements: Ontologies may need to evolve as new requirements emerge or the domain knowledge changes. Design your ontology to accommodate changes without significant rework.\nInteroperability: Ensuring that your ontology can interoperate with others is crucial but challenging, often requiring alignment or mapping between ontological terms.\nPerformance: Complex ontologies can impact the performance of data retrieval and reasoning. Optimization strategies may be necessary for large-scale applications."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#practical-guidance-for-biologists",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#practical-guidance-for-biologists",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Practical Guidance for Biologists",
    "text": "Practical Guidance for Biologists\n\nStart Small: Begin by defining a small, focused ontology that addresses a specific problem or data sharing challenge you frequently encounter.\nEngage with Peers: Collaborate with colleagues to validate and refine the ontology, ensuring it’s broadly applicable and valuable.\nLeverage Tools: Utilize ontology editing tools and graph databases that offer user-friendly interfaces and tutorials to ease the learning curve.\nIterate and Expand: Gradually expand the ontology and the knowledge graph as more data becomes available and new research questions arise."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#gene-ontology-unifying-biomedical-research",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#gene-ontology-unifying-biomedical-research",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Gene Ontology: Unifying Biomedical Research",
    "text": "Gene Ontology: Unifying Biomedical Research\nThe Gene Ontology (GO) project was initiated to create a consistent computational representation of gene and protein roles across various databases and research groups. Before GO, the description of gene functions was inconsistent and scattered, making it challenging to aggregate and compare findings from different studies.\n\nDevelopment Process\nCollaborative Effort: GO was developed by a consortium of researchers from multiple model organism databases, emphasizing a collaborative approach to ensure broad applicability and acceptance.\nStructured Framework: The ontology is divided into three main categories: biological processes, cellular components, and molecular functions, providing a comprehensive framework for gene function annotation.\nIterative Refinement: The ontology is continuously updated and refined based on community feedback and new scientific discoveries, demonstrating an adaptive and evolving approach to ontology management.\n\n\nChallenges\nData Heterogeneity: Integrating data from diverse sources required significant effort in standardization and mapping to ensure consistency across the ontology.\nKeeping Pace with Science: The rapid advancement of biomedical research necessitated an ongoing process to update and expand the ontology to capture new findings and concepts.\n\n\nAchievements and Applications\nEnhanced Data Sharing: GO enabled more effective data sharing and integration across various biological databases, facilitating comprehensive analyses.\nAdvanced Research: With a standardized framework for gene function, researchers could conduct large-scale comparative studies, meta-analyses, and integrative research, accelerating discoveries in genomics and proteomics.\nTool Development: The ontology underpins numerous bioinformatics tools and resources, aiding in gene annotation, function prediction, and the exploration of gene-disease associations."
  },
  {
    "objectID": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#environmental-ontology-envo-standardizing-environmental-data",
    "href": "posts/2024-04-09-How to: Controlled Vocabularies and Ontologies/index.html#environmental-ontology-envo-standardizing-environmental-data",
    "title": "Controlled Vocabularies, Ontologies and Graphs. Oh My!",
    "section": "Environmental Ontology (ENVO): Standardizing Environmental Data",
    "text": "Environmental Ontology (ENVO): Standardizing Environmental Data\nThe Environment Ontology (ENVO) is a structured vocabulary for the dynamic and diverse domains of environmental science. It provides terms and definitions to describe environments, habitats, and ecological conditions, facilitating the annotation, discovery, and integration of environmental data. Here’s a real example of how ENVO has been utilized in scientific research:\nCase Study: Monitoring Microbial Diversity in Varying Environments\nResearchers studying microbial diversity across different environments—such as forests, oceans, and urban areas—often face challenges in standardizing their metadata to enable effective data sharing and comparison. The diverse nature of environmental data, coupled with the lack of standardized vocabulary, historically made it difficult to aggregate and analyze findings from different studies.\nIn a study aiming to understand how microbial communities adapt to different environmental conditions, researchers used ENVO to annotate their samples with standardized terms. For instance, when studying soil samples from various geographical locations, they used ENVO terms to describe the specific type of soil, the surrounding environment, and other relevant ecological attributes. See their study here published in Frontiers in Microbiology.\nData Annotation: The researchers annotated each microbial dataset with ENVO terms. For example, samples taken from a tropical rainforest soil were tagged with ENVO terms corresponding to “tropical rainforest” and the specific type of soil.\nData Integration: This standardized annotation enabled the integration of their dataset with other studies that used ENVO, facilitating a meta-analysis of microbial diversity across different environmental types.\nDiscovery and Analysis: By using ENVO, the researchers could easily filter and compare datasets based on environmental attributes, enhancing their ability to detect patterns and correlations in microbial diversity across different habitats.\n\nResults and Impact\nThe use of ENVO allowed the researchers to:\nEnhance Comparability: By adopting a common vocabulary, datasets from different studies could be directly compared or combined, leading to more robust analyses and conclusions.\nFacilitate Interdisciplinary Research: The standardized environmental descriptors made it easier for researchers from different disciplines to collaborate, integrate their findings, and draw comprehensive insights about microbial ecology.\nSupport Data Discovery: The enriched metadata, standardized by ENVO, improved the discoverability of the researchers’ data, making it more accessible to other scientists and increasing the potential for future reuse in various environmental and ecological studies.\nThis example illustrates the practical benefits of using ontologies like ENVO in scientific research, where the standardization and structuring of data play crucial roles in enabling broader insights and fostering collaborative investigations across disciplines."
  },
  {
    "objectID": "posts/2023-06-23-Learning Python as an R User/index.html",
    "href": "posts/2023-06-23-Learning Python as an R User/index.html",
    "title": "Learning Python as an R User",
    "section": "",
    "text": "One of the main differences you’ll notice right away between Python and R is how difficult can be to install Python and get to the point where you’re writing code. Coming from R, it’s pretty straightforward and clear that you need to install the R language from the Comprehensive R Archive Network (CRAN), and then install R Studio and you’re basically off to the races. With Python, however, there’s a few different options. I managed to get it installed and running in VS Code using pyenv and poetry, but I needed a lot of help from a senior engineer to get it to actually work!\nAll these difficulties led me to using Python in Google Colaboratory for now. Using Colab abstracts away all the installation, environment and package management challenges, but with some trade-offs down the line if you’re interested in developing applications or using the same versions of packages and Python for reproducibility purposes. For now, it’s great to just get started understanding how the language works without worrying about all the other stuff.\n\n\nBoth languages are popular for data analysis but their distinct origin stories have resulted in some broad differences. R was developed by statisticians for statisticians as an evolution of the S programming language developed at Bell Labs by John Chambers and others in 1976. S had a strong focus on creating an interactive environment to make data analysis easier. As a result of this fundamental design decision, most experienced programmers find programming in R weird and confusing. R was released by Ross Lhaka and Robert Gentleman in 1996 as ‘free software’, as opposed to the commercial version of S, allowing the growth of language by a vibrant community. R’s strengths lay in the availability of numerous packages for statistical analysis, the ability to make ‘publication ready’ graphics, and the inclusive community that can support new programmers.\nVersion 1 of Python, on the other hand, was released in 1994, a couple years before the first version of R but almost 20 years later than S. An early focus for Python was using clean syntax, a feature that has made the language easier to learn and read compared to R. Although, the development of the pipe operator and the tidyverse in R has improved the readability substantially. But I digress. Python was built with simplicity in mind including principles such as ‘simple is better than complex’, ‘There should be one–and preferably only one–obvious way to do it’, and ‘flat is better than nested’. Those are three principles that really resonate with me as an R user given than there always seems to be a million ways to do the same thing in R and I always end up having to work with nested lists which are a pain to navigate and flatten!\nFrom my perspective, I would say that R is used more than Python in academia, especially in the fields of statistics and ecology. Python has a broader user-base, gaining traction in the ocean and atmospheric sciences and has without a doubt been more heavily used in the fields of machine learning and artificial intelligence. Python also has a reputation for having better support for web integration and deployment, though R has made strides in this space with the likes of the shiny package.\nBut, this blog post isn’t about which language is better or worse, or which you should or shouldn’t use. In practice it’s getting easier to use both languages within the same project thanks to packages like reticulate. Knowing the basics of both languages is nice if you’re working in the field of data science and will make you a better programmer, but I would argue it’s better to be an expert in one or the other rather than a novice or intermediate in both!\n\n\n\nNow, I’m going to get into the weeds a bit in terms of some of the technical differences between R and Python. Not really knowing any other programming language (apart from some C++ in high school), I was actually more surprised by the commonalities of the languages than the differences. I was expecting Python to feel very different and scary, but it turns out that quite a few of the programming paradigms in R translate well to Python.\nSo, fear Python not my useR friends.\n\n\nIn R we have:\n\nnumeric Includes integer (no decimal values) and double (has decimal values) numbers\nlogical TRUE or FALSE\ncharacter Strings of letters; always surrounded by quotes\nfactor Categorical variables. Ordered or un-ordered. Stored in memory as integers and names with character values.\ncomplex and raw also exist in R but are rarely used\n\nIn Python we have:\n\nnumeric Very similar to R and includes integers as well as decimal values called a float in Python.\nboolean TRUE or FALSE\nstring character strings declared using quotes\nnoneType A special type representing absence of a value or a null value\n\nThe main difference in R and Python in terms of Data Types is the lack of factors in Python and the presence of a noneType. However, using the pandas package, a categorical data type is introduced in Python.\n\n\n\nIn R we have:\n\nvector Can either be lists made up of different data types or atomic made up of the same data types\nmatrix A two dimensional data structure with elements of all the same class.\narray Can store data in any number of dimensions\ndataframe Typical spreadsheet style data table that is a list of atomic vectors which serve as columns. All columns must have the same number of rows and a column can only one contain one data type, though different columns can be different types (numeric, factor, etc).\n\nIn Python we have:\n\nlists Similar to R’s vectors. The are ordered collections of items which are mutable (can be changed) and can contain a mix of types (int, float, str, etc.). The can be indexed, concatenated and sliced.\ntuples Similar to lists but are immutable (can’t be changed after creation)\nsets Un-ordered collections of items.\ndictionaries Store key and value pairs. Similar to named lists in R where elements can be accessed using names rather than by their position.\ndataframes Python doesn’t natively support dataframes but the pandas library provides this structure and works similarly to dataframes in R.\narrays The numpy library provides a data structure that works like an array in R. These are often used for mathematical operations when efficiency is required.\n\n\n\n\nR starts counting at 1. Python starts counting at 0. That’s hard to get used to. So for example, to access the item at the start of a list in Python you would write my_list[0] whereas in R you would write my_list[1].\n\n\n\nIn R, sub setting vectors and dataframes can be accomplished many ways: $, subset(), [[, [ and the with functions to access elements of a data frame. It can be a bit confusing to know which one to use or read code that switches between the different methods.\nIn Python, the primary method of subsetting is using single square brackets: []. This works on strings, lists, dictionaries and tuples. You can get single elements back by indexing just one position, or get what Python calls a “slice” back by using a range. For example, my_string[1:3] returns a slice of the 2nd, 3rd, and 4th elements of my_string (remember zero indexing).\n\n\n\nIn R there are two assignment operators: = and &lt;- The equal sign is generally used in parameter definitions inside a function call ie my_func(a = 1) whereas the &lt;- assignment operator can be used in most (if not all) other instances.\nIn Python there’s just the good old equals sign for assignment =\n\n\n\nScoping refers to the visibility of a variable to other parts of code. Both R and Python use “lexical scoping”, but with a few key differences.\nIn R, the scope of a variable is determined by the environment in which it was created. R will first look into the current environment for that variable and if it’s not found it will continue to ‘go up a level’ to enclosing environments until it either finds the variable of the variable is not found in the global environment and eventually the system environment.\nIn Python, it’s fairly similar but there are a few extra scoping features to be aware of: you can chose to modify a global variable from within a function using the global keyword. For example:\nx = 10  # Here x is a global variable\n\n# define a function that modifys the global variable x\ndef modify_global():\n    global x  # We declare that we want the global x\n    x = 20  # This will change the global x\n\nprint(x)  # Output: 10\nmodify_global()\nprint(x)  # Output: 20\nIf you’re into writing nested functions, you can also chose which outer environment you would like to change using the nonlocal keyword so that you modify the variable in the immediately enclosing environment rather than the local environment or the global environment. For example:\ndef outer():\n    x = 10  # Here x is a variable local to the function outer, but non-local to the function inner\n    def inner():\n        nonlocal x  # We declare that we want the nonlocal x\n        x = 20  # This will change the nonlocal x\n\n    print(x)  # Output: 10\n    inner()\n    print(x)  # Output: 20\n\nouter()\nIn this example, the nonlocal keyword is used in the nested function inner to indicate that x refers to the x in the immediately enclosing scope, which is the function outer. Without the nonlocal keyword, x would be treated as a local variable within inner, and the assignment x = 20 would not affect the x in outer. This actually seems like a recipe for really confusing code :|\n\n\n\nR’s approach to OOP is more complex because it has not one, but five different OOP systems: S3, S4, RC, R6 and now R7.\nS3 and S4 are more function-oriented - methods belong to functions, not classes, unlike Python where methods belong to classes. RC is more like typical OOP in this respect in that methods belong to classes, but is rarely used in R. R6 is a package rather than part of base R, is similar to RC and was primarily developed for use with the Shiny package by Posit (R Studio). R7 was released quite recently and aims to consolidate the good parts of the various systems and simplify everyone’s lives. TBD if that happens ;)\nThe various implementations of OOP in R make it confusing. I admit I try to avoid getting to deep into the differences here, but you can end up in a world of confusion when these systems get intertwined.\nPython’s OOP:\nPython supports OOP with a simple, easy-to-understand syntax and structure. The key components of Python’s OOP are classes, objects, and methods. Critically, there is only one OOP system in Python!\n\nClasses are like blueprints for creating objects (a particular data structure). They define the properties (also called attributes) that the object should have and the methods (functions) that the object can perform.\nObjects are instances of a class, which can have attributes and methods.\nMethods are functions defined within a class, used to define the behaviours of the objects."
  },
  {
    "objectID": "posts/2023-06-23-Learning Python as an R User/index.html#general-differences-between-r-and-python",
    "href": "posts/2023-06-23-Learning Python as an R User/index.html#general-differences-between-r-and-python",
    "title": "Learning Python as an R User",
    "section": "",
    "text": "Both languages are popular for data analysis but their distinct origin stories have resulted in some broad differences. R was developed by statisticians for statisticians as an evolution of the S programming language developed at Bell Labs by John Chambers and others in 1976. S had a strong focus on creating an interactive environment to make data analysis easier. As a result of this fundamental design decision, most experienced programmers find programming in R weird and confusing. R was released by Ross Lhaka and Robert Gentleman in 1996 as ‘free software’, as opposed to the commercial version of S, allowing the growth of language by a vibrant community. R’s strengths lay in the availability of numerous packages for statistical analysis, the ability to make ‘publication ready’ graphics, and the inclusive community that can support new programmers.\nVersion 1 of Python, on the other hand, was released in 1994, a couple years before the first version of R but almost 20 years later than S. An early focus for Python was using clean syntax, a feature that has made the language easier to learn and read compared to R. Although, the development of the pipe operator and the tidyverse in R has improved the readability substantially. But I digress. Python was built with simplicity in mind including principles such as ‘simple is better than complex’, ‘There should be one–and preferably only one–obvious way to do it’, and ‘flat is better than nested’. Those are three principles that really resonate with me as an R user given than there always seems to be a million ways to do the same thing in R and I always end up having to work with nested lists which are a pain to navigate and flatten!\nFrom my perspective, I would say that R is used more than Python in academia, especially in the fields of statistics and ecology. Python has a broader user-base, gaining traction in the ocean and atmospheric sciences and has without a doubt been more heavily used in the fields of machine learning and artificial intelligence. Python also has a reputation for having better support for web integration and deployment, though R has made strides in this space with the likes of the shiny package.\nBut, this blog post isn’t about which language is better or worse, or which you should or shouldn’t use. In practice it’s getting easier to use both languages within the same project thanks to packages like reticulate. Knowing the basics of both languages is nice if you’re working in the field of data science and will make you a better programmer, but I would argue it’s better to be an expert in one or the other rather than a novice or intermediate in both!"
  },
  {
    "objectID": "posts/2023-06-23-Learning Python as an R User/index.html#technical-differences",
    "href": "posts/2023-06-23-Learning Python as an R User/index.html#technical-differences",
    "title": "Learning Python as an R User",
    "section": "",
    "text": "Now, I’m going to get into the weeds a bit in terms of some of the technical differences between R and Python. Not really knowing any other programming language (apart from some C++ in high school), I was actually more surprised by the commonalities of the languages than the differences. I was expecting Python to feel very different and scary, but it turns out that quite a few of the programming paradigms in R translate well to Python.\nSo, fear Python not my useR friends.\n\n\nIn R we have:\n\nnumeric Includes integer (no decimal values) and double (has decimal values) numbers\nlogical TRUE or FALSE\ncharacter Strings of letters; always surrounded by quotes\nfactor Categorical variables. Ordered or un-ordered. Stored in memory as integers and names with character values.\ncomplex and raw also exist in R but are rarely used\n\nIn Python we have:\n\nnumeric Very similar to R and includes integers as well as decimal values called a float in Python.\nboolean TRUE or FALSE\nstring character strings declared using quotes\nnoneType A special type representing absence of a value or a null value\n\nThe main difference in R and Python in terms of Data Types is the lack of factors in Python and the presence of a noneType. However, using the pandas package, a categorical data type is introduced in Python.\n\n\n\nIn R we have:\n\nvector Can either be lists made up of different data types or atomic made up of the same data types\nmatrix A two dimensional data structure with elements of all the same class.\narray Can store data in any number of dimensions\ndataframe Typical spreadsheet style data table that is a list of atomic vectors which serve as columns. All columns must have the same number of rows and a column can only one contain one data type, though different columns can be different types (numeric, factor, etc).\n\nIn Python we have:\n\nlists Similar to R’s vectors. The are ordered collections of items which are mutable (can be changed) and can contain a mix of types (int, float, str, etc.). The can be indexed, concatenated and sliced.\ntuples Similar to lists but are immutable (can’t be changed after creation)\nsets Un-ordered collections of items.\ndictionaries Store key and value pairs. Similar to named lists in R where elements can be accessed using names rather than by their position.\ndataframes Python doesn’t natively support dataframes but the pandas library provides this structure and works similarly to dataframes in R.\narrays The numpy library provides a data structure that works like an array in R. These are often used for mathematical operations when efficiency is required.\n\n\n\n\nR starts counting at 1. Python starts counting at 0. That’s hard to get used to. So for example, to access the item at the start of a list in Python you would write my_list[0] whereas in R you would write my_list[1].\n\n\n\nIn R, sub setting vectors and dataframes can be accomplished many ways: $, subset(), [[, [ and the with functions to access elements of a data frame. It can be a bit confusing to know which one to use or read code that switches between the different methods.\nIn Python, the primary method of subsetting is using single square brackets: []. This works on strings, lists, dictionaries and tuples. You can get single elements back by indexing just one position, or get what Python calls a “slice” back by using a range. For example, my_string[1:3] returns a slice of the 2nd, 3rd, and 4th elements of my_string (remember zero indexing).\n\n\n\nIn R there are two assignment operators: = and &lt;- The equal sign is generally used in parameter definitions inside a function call ie my_func(a = 1) whereas the &lt;- assignment operator can be used in most (if not all) other instances.\nIn Python there’s just the good old equals sign for assignment =\n\n\n\nScoping refers to the visibility of a variable to other parts of code. Both R and Python use “lexical scoping”, but with a few key differences.\nIn R, the scope of a variable is determined by the environment in which it was created. R will first look into the current environment for that variable and if it’s not found it will continue to ‘go up a level’ to enclosing environments until it either finds the variable of the variable is not found in the global environment and eventually the system environment.\nIn Python, it’s fairly similar but there are a few extra scoping features to be aware of: you can chose to modify a global variable from within a function using the global keyword. For example:\nx = 10  # Here x is a global variable\n\n# define a function that modifys the global variable x\ndef modify_global():\n    global x  # We declare that we want the global x\n    x = 20  # This will change the global x\n\nprint(x)  # Output: 10\nmodify_global()\nprint(x)  # Output: 20\nIf you’re into writing nested functions, you can also chose which outer environment you would like to change using the nonlocal keyword so that you modify the variable in the immediately enclosing environment rather than the local environment or the global environment. For example:\ndef outer():\n    x = 10  # Here x is a variable local to the function outer, but non-local to the function inner\n    def inner():\n        nonlocal x  # We declare that we want the nonlocal x\n        x = 20  # This will change the nonlocal x\n\n    print(x)  # Output: 10\n    inner()\n    print(x)  # Output: 20\n\nouter()\nIn this example, the nonlocal keyword is used in the nested function inner to indicate that x refers to the x in the immediately enclosing scope, which is the function outer. Without the nonlocal keyword, x would be treated as a local variable within inner, and the assignment x = 20 would not affect the x in outer. This actually seems like a recipe for really confusing code :|\n\n\n\nR’s approach to OOP is more complex because it has not one, but five different OOP systems: S3, S4, RC, R6 and now R7.\nS3 and S4 are more function-oriented - methods belong to functions, not classes, unlike Python where methods belong to classes. RC is more like typical OOP in this respect in that methods belong to classes, but is rarely used in R. R6 is a package rather than part of base R, is similar to RC and was primarily developed for use with the Shiny package by Posit (R Studio). R7 was released quite recently and aims to consolidate the good parts of the various systems and simplify everyone’s lives. TBD if that happens ;)\nThe various implementations of OOP in R make it confusing. I admit I try to avoid getting to deep into the differences here, but you can end up in a world of confusion when these systems get intertwined.\nPython’s OOP:\nPython supports OOP with a simple, easy-to-understand syntax and structure. The key components of Python’s OOP are classes, objects, and methods. Critically, there is only one OOP system in Python!\n\nClasses are like blueprints for creating objects (a particular data structure). They define the properties (also called attributes) that the object should have and the methods (functions) that the object can perform.\nObjects are instances of a class, which can have attributes and methods.\nMethods are functions defined within a class, used to define the behaviours of the objects."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decode the Overload",
    "section": "",
    "text": "Controlled Vocabularies, Ontologies and Graphs. Oh My!\n\n\n\n\n\n\n\nHow-to\n\n\nPython\n\n\nOntology\n\n\nData Science\n\n\n\n\nTake your glossary of terms to the next level by creating an ontology from a spreadsheet using Python, RDFlib, and Web Protege.\n\n\n\n\n\n\nApr 9, 2024\n\n\nBrett Johnson\n\n\n\n\n\n\n  \n\n\n\n\nGoogle’s Business Intelligence Certificate Overview\n\n\n\n\n\n\n\nBusiness Intelligence\n\n\nGoogle\n\n\nProfessional Development\n\n\n\n\nBusiness Intelligence Methods for Scientific Data Mobilization\n\n\n\n\n\n\nSep 5, 2023\n\n\nBrett Johnson\n\n\n\n\n\n\n  \n\n\n\n\nHow Does the Global Biodiversity Information System Count Citations from the International Year of the Salmon?\n\n\n\n\n\n\n\nBiodiversity\n\n\nGBIF\n\n\nDatacite\n\n\nInternational Year of the Salmon\n\n\n\n\nHow to use the DataCite REST API to understand how GBIF counts citations\n\n\n\n\n\n\nJun 20, 2023\n\n\nBrett Johnson\n\n\n\n\n\n\n  \n\n\n\n\nHow do we mobilize salmon data?\n\n\n\n\n\n\n\nSalmon\n\n\necology\n\n\n\n\nCommunicate, assemble and coordinate extant data systems\n\n\n\n\n\n\nMay 30, 2023\n\n\nBrett Johnnson\n\n\n\n\n\n\n  \n\n\n\n\nLearning Python as an R User\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nData Science\n\n\n\n\nPython versus R? Nah Python + R\n\n\n\n\n\n\nJun 23, 2022\n\n\nBrett Johnson\n\n\n\n\n\n\nNo matching items"
  }
]