[
  {
    "objectID": "posts/2023-05-30-Mobilizing Salmon Data/index.html",
    "href": "posts/2023-05-30-Mobilizing Salmon Data/index.html",
    "title": "How do we mobilize salmon data?",
    "section": "",
    "text": "There exists a mountain of data about salmon, their environment, and abundance yet we struggle to understand why or predict when certain populations of salmon decline. Part of the challenge is that salmon have a complicated life history where they are exposed to multiple environments throughout their lives. Some parts of their lives are easier to observe than others and some environments are more complex than others. The challenge is to figure out what observations have the most predictive power in each life phase, and then put all those data together to glean a complete and predictive history of the conditions encountered by specific salmon stocks.\nHowever, because salmon cross arbitrary municipal, provincial, and national borders we lack a coordinated approach to aggregating data. Data often are collected using bespoke standards, stored locally, and not shared. Data management is not a trivial task. Therefore, any coordinated approach needs to be lightweight and flexible such that the barrier to adherence is surmountable by individual biologists and fall in line with the basic tenets of FAIR data.\nIndeed, the key to mobilizing salmon data doesn’t lie in creating new technologies. Rather, it rests on effectively harnessing and integrating existing ones, ensuring they are user-friendly, supporting their broad adoption, and raising awareness about their use. It’s essential to remember that adhering to practical principles and modern best-practices that solve immediate problems will propel us further faster than being distracted by the allure of novel technologies or flashy platforms. Success hinges on assisting agencies to meet their data publishing mandates. To achieve this, we must address challenges faced by individual biologists and data managers one at a time, providing solutions rooted in practicality and principles that are underpinned by well-adopted global best-practices, and aligning these with existing national mandates. By sticking to the fundamentals outlined below, the salmon research community will be well-positioned to effectively manage, integrate, and harmonize the vast amount of data being produced to understand, predict and manage changes to salmon ecoystems."
  },
  {
    "objectID": "posts/2023-05-30-Mobilizing Salmon Data/index.html#proof-of-concept",
    "href": "posts/2023-05-30-Mobilizing Salmon Data/index.html#proof-of-concept",
    "title": "How do we mobilize salmon data?",
    "section": "Proof of Concept",
    "text": "Proof of Concept\nLet’s look through DOIs registered with DataCite that have the word salmon somewhere in the digital objects metadata. To do this I will use DataCite’s REST API and the R package rdatacite which provides convenient functions for making API calls.\n\n\nCode\nlibrary(rdatacite)\nlibrary(tidyverse)\nlibrary(plotly)\n# Initialize lists to store dataframes and metadata\nall_datasets &lt;- list()\nall_meta &lt;- list()\n\n# Fetch initial page\ntt_salmon_datasets &lt;- dc_dois(query = \"salmon\")\n\ntt_providers &lt;- tt_salmon_datasets[[\"meta\"]][[\"providers\"]]\ntt_affiliations &lt;- tt_salmon_datasets[[\"meta\"]][[\"affiliations\"]][[\"title\"]]\n\nplotly::ggplotly(ggplot2::ggplot(tt_providers, aes(x = reorder(title, count), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(x = \"Publisher\", y = \"Count\") +\n  theme_minimal() +\n  ggtitle(\"Top Ten Publishers of Salmon Datasets\")) \n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\ntt_affiliations &lt;- tt_salmon_datasets[[\"meta\"]][[\"affiliations\"]]\n\nggplot(tt_affiliations, aes(x = reorder(title, count), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(x = \"Publisher\", y = \"Count\") +\n  theme_minimal() +\n  ggtitle(\"Top Ten Institutions Providing Salmon Datasets\")\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\nyears_published &lt;- tt_salmon_datasets[[\"meta\"]][[\"published\"]]\n\nggplot(years_published, ggplot2::aes(x = id, y = count))+\n                   geom_bar(stat = \"identity\", fill = \"steelblue\")+\n                   labs(x = \"Year\", y = \"Count\") +\n                   theme_minimal() +\n                   ggtitle(\"Growth of salmon datasets published with a DOI\")"
  },
  {
    "objectID": "posts/2023-06-22-How Does GBIF Count Citations?/gbif.html",
    "href": "posts/2023-06-22-How Does GBIF Count Citations?/gbif.html",
    "title": "How Does GBIF Count Citations?",
    "section": "",
    "text": "Introduction\nIf you publish your data to the Global Biodiversity Information Facility (GBIF), you may notice that your dataset is being cited by papers that seemingly have nothing to do with the context of your original project that produced the dataset.\nI had a colleague involved with the International Year of the Salmon (IYS) ask why IYS datasets such as one dataset about zooplankton was referenced in a paper that seemed to be about terrestrial species [@garcía-roselló2023].\nIf you look in the references of the Garcia-Rosello et al 2023 paper you will see two links to GBIF Occurrence Downloads. If you click on the DOI for GBIF.org, 2022b (https://doi.org/10.15468/dl.cqpa99), you will reach a landing page for an Occurrence Download from GBIF from a specific query. You can see the specific query filters that were used to generate the dataset.\nA total of 18,426 datasets that meet the query filter. The filter included: has CC BY 4.0 licence, Basis of record must be either Human observation, Observation or Preserved Specimen, Has coordinate is true, … Scientific name includes Animalia). As it turns out, this is a very broad filter basically asking for all the animal occurrences on GBIF datasets that have the right license and metadata and the dataset about zooplankton met all those criteria. If you wanted to see all the datasets included in the new dataset you can download a list of the involved datasets using the “Download as TSV” link from the GBIF Occurrence Dataset landing page for this big dataset and search for International Year of the Salmon to see which datasets are included.\n\nSo the dataset that resulted from this specific query includes occurrences meeting the query filter parameters from 18,426 other GBIF datasets receives a Digital Object Identifier. Each of those underlying datasets also has a digital object identifier and each of those DOIs is referenced in the new dataset’s DOI metadata as a ‘relatedIdentifier’.\n\nSo, each time the overarching dataset is cited, the citation count for each of the 18,426 datasets increases by one as well. Pretty cool, huh!?\n\nI can be surprising how the International Year of the Salmon data are re-used way outside the context of their collection, which is a major advantage of using the GBIF and publishing data using international standards.\nHere’s how you can access the metadata about the overarching DOI as well to see what kind of datasets it included.\n\n\nCode\nlibrary(rdatacite)\nlibrary(tidyverse)\n\n# download the DOI metadata for the new overarching dataset\ngbif_doi &lt;- dc_dois(ids = \"10.15468/dl.cqpa99\")\n\ndatasets_used &lt;- gbif_doi[[\"data\"]][[\"attributes\"]][[\"relatedIdentifiers\"]][[1]][\"relatedIdentifier\"]\n\n\nUsing this list of DOIs that the overarching dataset uses, we could retrieve all the names of those datasets, but let’s do something a bit more interesting.\nLet’s look at the International Year of the Salmon datasets and how they’ve been utilized by GBIF.\n\n\nCode\niys_dois &lt;- dc_dois(query = \"titles.title:International Year of the Salmon\", limit = 1000)\n\niys_citations &lt;- tibble(title = lapply(iys_dois$data$attributes$titles, \"[[\", \"title\"), citations = iys_dois[[\"data\"]][[\"attributes\"]][[\"citationCount\"]], doi = iys_dois[[\"data\"]][[\"attributes\"]][[\"doi\"]]) |&gt; \n  filter(citations &gt; 0)\n\niys_citations$title &lt;- substr(iys_citations$title, 4, 80)\n\ncites_iys_list &lt;- list()\n\nfor (i in iys_citations$doi) {\n  \n  x &lt;- dc_events(obj_id = paste0(\"https://doi.org/\", i))\n  cites_iys_list[[i]] &lt;- x\n}\n\n# Initialize lists to store objId and subjId\nobj_ids &lt;- list()\nsubj_ids &lt;- list()\n\n# Loop over the list to retrieve objId and subjId\nfor(i in 1:length(cites_iys_list)) {\n  data &lt;- cites_iys_list[[i]]$data$attributes\n  obj_ids[[i]] &lt;- data$objId\n  subj_ids[[i]] &lt;- data$subjId\n}\n\n# Flatten the lists\nobj_ids &lt;- unlist(obj_ids)\nobj_ids &lt;- substring(obj_ids, 17)\nsubj_ids &lt;- unlist(subj_ids)\nsubj_ids &lt;- substring(subj_ids, 17)\n\n# Get titles for objId and subjId\nobj_titles &lt;- rdatacite::dc_dois(ids = obj_ids, limit = 1000)\nsubj_titles &lt;- rdatacite::dc_dois(ids = subj_ids, limit = 1000)\n\nobj_dois &lt;- obj_titles[[\"data\"]][[\"attributes\"]][[\"doi\"]]\ntitle_list &lt;- obj_titles[[\"data\"]][[\"attributes\"]][[\"titles\"]]\ntitle_vector &lt;- unlist(map(title_list, function(x) x[['title']][1]))\nseq &lt;- as.character(1:length(obj_dois))\nobjects &lt;- tibble(position = seq, obj_dois, title_vector)\n\nsubj_dois &lt;- subj_titles[[\"data\"]][[\"attributes\"]][[\"doi\"]]\nsubjtitle_list &lt;- subj_titles[[\"data\"]][[\"attributes\"]][[\"titles\"]]\nsubjtitle_vector &lt;- unlist(map(subjtitle_list, function(x) x[['title']][1]))\nseq2 &lt;- as.character(1:length(subj_dois))\nsubjects &lt;- tibble(seq2, subj_dois, subjtitle_vector) |&gt; \n  filter(subjtitle_vector != \"Zooplankton Bongo Net Data from the 2019 and 2020 Gulf of Alaska International Year of the Salmon Expeditions\")\n\nsubj_related_ids &lt;- bind_rows(subj_titles[[\"data\"]][[\"attributes\"]][[\"relatedIdentifiers\"]], .id = \"position\") |&gt; \n  semi_join(objects, by = c('relatedIdentifier' = 'obj_dois')) |&gt; \n  left_join(subjects, by = c('position' = 'seq2')) |&gt; \n  filter(relationType != \"IsPreviousVersionOf\")\n  \n\n\nrelationships &lt;- full_join(objects, subj_related_ids, by = c('obj_dois' = 'relatedIdentifier'))\n\n\nlibrary(networkD3)\n\nobjects$type.label &lt;- \"IYS Dataset\"\nsubjects$type.label &lt;- \"Referencing Dataset or Paper\"\nids &lt;- c(objects$obj_dois,subjects$subj_dois)\nnames &lt;- c(objects$title_vector, subjects$subjtitle_vector)\ntype.label &lt;- c(objects$type.label, subjects$type.label)\n\nedges &lt;-tibble(from = relationships$obj_dois, to = relationships$subj_dois)\nlinks.d3 &lt;- data.frame(from=as.numeric(factor(edges$from))-1, \n                       to=as.numeric(factor(edges$to))-1 ) \nsize &lt;- links.d3 |&gt; \n   group_by(from) |&gt; \n  summarize(weight = n())\n\nnodes &lt;- tibble(ids, names, type.label) |&gt; \n  mutate(names = case_when(\n    names == \"Occurrence Download\" ~ paste0(names, \" \", ids),\n    TRUE ~ names\n    ),\n  )\n\nlength &lt;- nrow(nodes)\n\nmissing_length &lt;- as.integer(length) - nrow(size)\nmissing_size &lt;- rep.int(0, missing_length)\nsize &lt;- c(size$weight, missing_size)\n\nnodes$size &lt;- size\n\nnodes.d3 &lt;- cbind(idn=factor(nodes$names, levels=nodes$names), nodes) \n\n\n\n\nCode\nlibrary(networkD3)\nplot &lt;- forceNetwork(Links = links.d3, Nodes = nodes.d3, Source=\"from\", Target=\"to\",\n               NodeID = \"idn\", Group = \"type.label\",linkWidth = 1,\n               linkColour = \"#afafaf\", fontSize=12, zoom=T, legend=T, \n               Nodesize=\"size\", opacity = 0.8, charge=-300,\n               width = 600, height = 400)\n\nplot"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decode the Overload",
    "section": "",
    "text": "How Does GBIF Count Citations?\n\n\n\n\n\n\n\nHow-to\n\n\nR\n\n\nRmarkdown\n\n\n\n\ndescription\n\n\n\n\n\n\nJun 22, 2023\n\n\nBrett Johnson\n\n\n\n\n\n\n  \n\n\n\n\nHow do we mobilize salmon data?\n\n\n\n\n\n\n\nSalmon\n\n\necology\n\n\n\n\nCommunicate, assemble and coordinate extant data systems\n\n\n\n\n\n\nMay 30, 2023\n\n\nBrett Johnnson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]